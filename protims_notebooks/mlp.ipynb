{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf;\n",
    "from tensorflow import python as tf_python\n",
    "import numpy as np;\n",
    "import time;\n",
    "import os;\n",
    "import sys;\n",
    "\n",
    "DTYPE = tf.float32;\n",
    "EPS = np.finfo(np.double).eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We got a GPU\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    print(\"We got a GPU\")\n",
    "    print(physical_devices)\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "else:\n",
    "    print(\"Sorry, no GPU for you...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def and_data():\n",
    "    input_array = np.array( [ [ 0.0, 0.0 ], [ 0.0, 1.0 ], [ 1.0, 0.0 ], [ 1.0, 1.0 ] ] );\n",
    "    output_array = np.array( [ [ 0.0 ], [ 0.0 ], [ 0.0 ], [ 1.0 ] ] );\n",
    "    return ( input_array.astype('float32'), output_array.astype('float32') ),\\\n",
    "( input_array.astype('float32'), output_array.astype('float32') );\n",
    "    \n",
    "def or_data():\n",
    "    input_array = np.array( [ [ 0.0, 0.0 ], [ 0.0, 1.0 ], [ 1.0, 0.0 ], [ 1.0, 1.0 ] ] );\n",
    "    output_array = np.array( [ [ 0.0 ], [ 1.0 ], [ 1.0 ], [ 1.0 ] ] );\n",
    "    return ( input_array.astype('float32'), output_array.astype('float32') ),\\\n",
    "( input_array.astype('float32'), output_array.astype('float32') );\n",
    "    \n",
    "def xor_data():\n",
    "    input_array = np.array( [ [ 0.0, 0.0 ], [ 0.0, 1.0 ], [ 1.0, 0.0 ], [ 1.0, 1.0 ] ] );\n",
    "    output_array = np.array( [ [ 0.0 ], [ 1.0 ], [ 1.0 ], [ 0.0 ] ] );\n",
    "    return ( input_array.astype('float32'), output_array.astype('float32') ),\\\n",
    "( input_array.astype('float32'), output_array.astype('float32') );\n",
    "\n",
    "def xor_rolled_data():\n",
    "    input_array = np.array( [ [ 0.0, 0.0 ], [ 0.0, 1.0 ], [ 1.0, 0.0 ], [ 1.0, 1.0 ] ] );\n",
    "    output_array = np.array( [ [ 0.2 ], [ 0.8 ], [ 0.8 ], [ 0.2 ] ] );\n",
    "    return ( input_array.astype('float32'), output_array.astype('float32') ),\\\n",
    "( input_array.astype('float32'), output_array.astype('float32') );\n",
    "    \n",
    "def not_data():\n",
    "    # compute not x_0\n",
    "    input_array = np.array( [ [ 0.0, 0.0 ], [ 0.0, 1.0 ], [ 1.0, 0.0 ], [ 1.0, 1.0 ] ] );\n",
    "    output_array = np.array( [ [ 1.0 ], [ 1.0 ], [ 0.0 ], [ 0.0 ] ] );\n",
    "    return ( input_array.astype('float32'), output_array.astype('float32') ),\\\n",
    "( input_array.astype('float32'), output_array.astype('float32') );\n",
    "\n",
    "\n",
    "def extended_and_data():\n",
    "    input_array = np.array( [ [ 0.1, 0.1, 0.1 ], [ 0.1, 0.1, 0.9 ], [ 0.1, 0.9, 0.1 ], [ 0.1, 0.9, 0.9 ] , [ 0.9, 0.1, 0.1 ],  [ 0.9, 0.1, 0.9 ],  [ 0.9, 0.9, 0.1 ],  [ 0.9, 0.9, 0.9 ] ]);\n",
    "    output_array = np.array( [ [ 0.1 ], [ 0.1 ], [ 0.1 ], [ 0.1 ], [ 0.1 ], [ 0.1 ], [ 0.1 ], [ 0.9 ] ] );\n",
    "    #input_array = np.array( [ [ 0.1, 0.1, 0.1 ], [ 0.1, 0.1, 0.9 ], [ 0.1, 0.9, 0.1 ], [ 0.1, 0.9, 0.9 ] ]);\n",
    "    #output_array = np.array( [ [ 0.1 ], [ 0.1 ], [ 0.1 ], [ 0.1 ] ] );\n",
    "    return ( input_array.astype('float32'), output_array.astype('float32') ),\\\n",
    "( input_array.astype('float32'), output_array.astype('float32') );\n",
    "    \n",
    "def extended_or_data():\n",
    "    input_array = np.array( [ [ 0.1, 0.1, 0.1 ], [ 0.1, 0.1, 0.9 ], [ 0.1, 0.9, 0.1 ], [ 0.1, 0.9, 0.9 ] , [ 0.9, 0.1, 0.1 ],  [ 0.9, 0.1, 0.9 ],  [ 0.9, 0.9, 0.1 ],  [ 0.9, 0.9, 0.9 ] ]);\n",
    "    #input_array = np.array( [ [ 0.1, 0.1, 0.1 ], [ 0.1, 0.1, 0.9 ], [ 0.1, 0.9, 0.1 ], [ 0.1, 0.9, 0.9 ] ]);\n",
    "    output_array = np.array( [ [ 0.1 ], [ 0.9 ], [ 0.9 ], [ 0.9 ], [ 0.9 ], [ 0.9 ], [ 0.9 ], [ 0.9 ] ] );\n",
    "    \n",
    "    return ( input_array.astype('float32'), output_array.astype('float32') ),\\\n",
    "( input_array.astype('float32'), output_array.astype('float32') );\n",
    "    \n",
    "def extended_xor_data():\n",
    "    input_array = np.array( [ [ 0.1, 0.1, 0.1 ], [ 0.1, 0.1, 0.9 ], [ 0.1, 0.9, 0.1 ] ]);\n",
    "    output_array = np.array( [ [ 0.1 ], [ 0.9 ], [ 0.9 ] ] );\n",
    "    return ( input_array.astype('float32'), output_array.astype('float32') ),\\\n",
    "( input_array.astype('float32'), output_array.astype('float32') );\n",
    "\n",
    "\n",
    "def load_data( name ):\n",
    "    if name == \"and\":\n",
    "        return and_data();\n",
    "    if name == \"or\":\n",
    "        return or_data();\n",
    "    if name == \"xor\":\n",
    "        return xor_data();\n",
    "    if name == \"not\":\n",
    "        return not_data();\n",
    "    if name == \"xor_rolled\":\n",
    "        return xor_rolled_data();\n",
    "    if name == \"extended_and\":\n",
    "        return extended_and_data();\n",
    "    if name == \"extended_or\":\n",
    "        return extended_or_data();\n",
    "    if name == \"extended_xor\":\n",
    "        return extended_xor_data();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "\n",
    "class AbstractLayer(metaclass=abc.ABCMeta):\n",
    "    @abc.abstractmethod\n",
    "    def output(self, input):\n",
    "        \"\"\"Compute the activation values of the neurons in this Layer.\"\"\"\n",
    "        raise NotImplementedError(\"Must override output method\")\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def update(self, loss, tape, optimizer):\n",
    "        \"\"\"Update the loss for the layer.\"\"\"\n",
    "        raise NotImplementedError(\"Must override update method\")\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def gradientDescent_update(self, loss, tape):\n",
    "        \"\"\"Update the gradient according to the tape and the loss.\"\"\"\n",
    "        raise NotImplementedError(\"Must override gradientDescent_update method\")\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def get_gradient(self, loss, tape):\n",
    "        \"\"\"Get the gradient according to the tape.\"\"\"\n",
    "        raise NotImplementedError(\"Must override get_gradient method\")\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the layer.\"\"\"\n",
    "        raise NotImplementedError(\"Must override reset method\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from typing import Callable, Dict\n",
    "\n",
    "class BaseLayer(AbstractLayer):\n",
    "    \"\"\"A layer in a neural network.\"\"\"\n",
    "\n",
    "    def __init__(self, params: Dict, name: str, node_no: int, transfer: Callable):\n",
    "        \"\"\"Initialize the layer.\"\"\"\n",
    "        self.params = params\n",
    "        self.name = name\n",
    "        self.node_no = node_no\n",
    "        self.transfer = transfer\n",
    "        self.inputs = None\n",
    "        self.outputs = None\n",
    "        self.biases   = tf.Variable( \n",
    "                              self.params['BIAS_INIT']( [ self.node_no ], dtype=DTYPE ),             \n",
    "                              name='B_%s' % (self.name,),\n",
    "                              trainable=True );\n",
    "\n",
    "    def __rshift__(self, other):\n",
    "        \"\"\"Connect two layers together.\"\"\"\n",
    "        print(other)\n",
    "\n",
    "    @tf.function\n",
    "    def output(self, input):\n",
    "        \"\"\"Compute the activation values of the neurons in this Layer.\"\"\"\n",
    "        print( self.name, self.inputs.output(input).shape, self.biases.shape)\n",
    "        return self.transfer(tf.nn.bias_add(self.inputs.output(input), self.biases))\n",
    "\n",
    "    def gradientDescent_update(self, loss, tape):\n",
    "        \"\"\"Update the biases according to the gradient descent bias update.\"\"\"\n",
    "        gradient = self.get_gradient(loss, tape)\n",
    "        self.biases.assign(self.biases - self.params['LEARNING_RATE'] * gradient)\n",
    "\n",
    "    def get_gradient(self, loss, tape):\n",
    "        \"\"\"Return the gradient from the tape.\"\"\"\n",
    "        return tape.gradient(loss, self.biases)\n",
    "\n",
    "    def update(self, loss, tape, optimizer):\n",
    "        \"\"\"Update the biases according to the optimizer object's bias update rule.\"\"\"\n",
    "        gradient = self.get_gradient(loss, tape)\n",
    "        optimizer.apply_gradients(zip([gradient], [self.biases]))\n",
    "\n",
    "    def alopex_update(self, optimizer):\n",
    "        \"\"\"Update the biases according to the Alopex algorithm.\"\"\"\n",
    "        self.biases.assign_add(self.params['ALOPEX_LEARNING_RATE'] * tf.cast(optimizer.update_x[self.biases.name], dtype=DTYPE))\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the bias.\"\"\"\n",
    "        self.biases.assign(self.params['BIAS_INIT']([self.node_no], dtype=DTYPE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(BaseLayer):\n",
    "    \"\"\"A layer in a neural network.\"\"\"\n",
    "\n",
    "    def __init__(self, params: Dict, name: str, node_no: int, transfer: Callable):\n",
    "        \"\"\"Initialize the layer.\"\"\"\n",
    "        super().__init__(params, name, node_no, transfer)\n",
    "\n",
    "    def __rshift__(self, other):\n",
    "        \"\"\"Create a neural network by connecting two layers together.\"\"\"\n",
    "        return NN(self, other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputLayer(Layer):\n",
    "    \"\"\"An input layer in a neural network.\"\"\"\n",
    "\n",
    "    def __init__(self, params: Dict, name: str, node_no: int):\n",
    "        \"\"\"Initialize the layer.\"\"\"\n",
    "        self.params = params\n",
    "        self.name = name\n",
    "        self.node_no = node_no\n",
    "\n",
    "    @tf.function\n",
    "    def output(self, inputs):\n",
    "        \"\"\"Return the inputs as the output of this layer.\"\"\"\n",
    "        return inputs\n",
    "\n",
    "    def update(self, loss, tape, optimizer):\n",
    "        \"\"\"No update is needed for the input layer.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def gradientDescent_update(self, loss, tape):\n",
    "        \"\"\"No gradient descent update is needed for the input layer.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def get_gradient(self, loss, tape):\n",
    "        \"\"\"No gradient is needed for the input layer.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def alopex_update(self, optimizer):\n",
    "        \"\"\"No Alopex update is needed for the input layer.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"No reset is needed for the input layer.\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightLayer(BaseLayer):\n",
    "    \"\"\"A layer of weights in a neural network.\"\"\"\n",
    "\n",
    "    def __init__(self, params: Dict, src: Layer, dest: Layer):\n",
    "        self.params  = params;\n",
    "        self.src     = src;\n",
    "        # make init a function that will randomize weights\n",
    "        self.name    = \"W_%s_%s\" % (src.name,dest.name);\n",
    "        \n",
    "        self.dest    = dest;\n",
    "        self.weights = tf.Variable( \n",
    "                           self.params['WEIGHT_INIT']( [ src.node_no, \n",
    "                                                         dest.node_no ], \n",
    "                                        dtype=DTYPE ), \n",
    "                           name=self.name,\n",
    "                              trainable=True  );\n",
    "        \n",
    "        self.src.outputs = self;\n",
    "        self.dest.inputs = self;\n",
    "\n",
    "    @tf.function\n",
    "    def output(self, inputs):\n",
    "        \"\"\"Compute the output of this layer.\"\"\"\n",
    "        print( self.name,self.src.name, self.dest.name,self.src.output(inputs).shape,self.weights.shape)\n",
    "        return tf.matmul(self.src.output(inputs), self.weights)\n",
    "\n",
    "    def get_gradient(self, loss, tape):\n",
    "        \"\"\"Compute the gradient of the loss with respect to the weights.\"\"\"\n",
    "        return tape.gradient(loss, self.weights)\n",
    "\n",
    "    def gradientDescent_update(self, loss, tape):\n",
    "        \"\"\"Update the weights using the gradient descent algorithm.\"\"\"\n",
    "        gradient = self.get_gradient(loss, tape)\n",
    "        self.weights.assign(self.weights - self.params['LEARNING_RATE'] * gradient)\n",
    "\n",
    "    def update(self, loss, tape, optimizer):\n",
    "        \"\"\"Update the weights using the provided optimizer.\"\"\"\n",
    "        gradient = self.get_gradient(loss, tape)\n",
    "        print(gradient)\n",
    "        print(self.weights)\n",
    "        optimizer.apply_gradients(zip([gradient], [self.weights]))\n",
    "\n",
    "    def alopex_update(self, optimizer):\n",
    "        \"\"\"Update the weights using the Alopex algorithm.\"\"\"\n",
    "        self.weights.assign_add(\n",
    "            self.params['ALOPEX_LEARNING_RATE'] * tf.cast(optimizer.update_x[self.weights.name], dtype=DTYPE)\n",
    "        )\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the weights to their initial values.\"\"\"\n",
    "        self.weights.assign(\n",
    "            self.params['WEIGHT_INIT']([self.src.node_no, self.dest.node_no], dtype=DTYPE)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:\n",
    "    \"\"\"\n",
    "    This class represents a Neural Network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layer_0, layer_1):\n",
    "        \"\"\"\n",
    "        Construct a network with an input layer, layer_0, and one more \n",
    "        layer, layer_1.\n",
    "        \"\"\"\n",
    "        self.params = layer_0.params\n",
    "        self.layers = [layer_0, layer_1]\n",
    "        self.input_layer = self.layers[0]\n",
    "        self.weights = [WeightLayer(self.params, layer_0, layer_1)]\n",
    "        self.output_layer = self.layers[-1]\n",
    "        self.trainable_variables = [self.weights[0].weights, self.output_layer.biases]\n",
    "        self.loss_fn = None\n",
    "        self.optimizer = None\n",
    "\n",
    "    def __rshift__(self, other):\n",
    "        \"\"\"\n",
    "        Add a Layer to the network.\n",
    "        \"\"\"\n",
    "        if isinstance(other, Layer):\n",
    "            self.layers.append(other)\n",
    "            self.weights.append(WeightLayer(self.params, self.layers[-2], other))\n",
    "            self.output_layer = self.layers[-1]\n",
    "            self.trainable_variables.extend([self.weights[-1].weights, self.output_layer.biases])\n",
    "            return self\n",
    "\n",
    "        if isinstance(other, tf_python.eager.polymorphic_function.polymorphic_function.Function):\n",
    "            self.loss_fn = other\n",
    "            return self\n",
    "\n",
    "        if isinstance(other, str):\n",
    "            self.optimizer = other\n",
    "            return self\n",
    "\n",
    "        raise TypeError(f\"Unsupported type: {type(other)}\")\n",
    "\n",
    "    @tf.function\n",
    "    def confusion(self, targets, inputs):\n",
    "        return tf.math.confusion_matrix(targets, tf.reshape(tf.math.round(self.output(inputs)), [targets.shape[0]]))\n",
    "\n",
    "    @tf.function\n",
    "    def output(self, inputs):\n",
    "        \"\"\"\n",
    "        Compute the output values generated by the NN for a given input.\n",
    "        \"\"\"\n",
    "        return self.output_layer.output(inputs)\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset all the weights and biases.\n",
    "        \"\"\"\n",
    "        for weight in self.weights:\n",
    "            weight.reset()\n",
    "\n",
    "        for layer in self.layers:\n",
    "            layer.reset()\n",
    "\n",
    "    @tf.function\n",
    "    def loss(self, target, inputs):\n",
    "        \"\"\"\n",
    "        Apply the loss function to compute the loss.\n",
    "        \"\"\"\n",
    "        return self.loss_fn(target, self.output(inputs))\n",
    "\n",
    "    def train(self, target, inputs, epoch, mini_batch):\n",
    "        \"\"\"\n",
    "        Propagate the inputs through the network and\n",
    "        update the weights and layers (biases) based on\n",
    "        the target values.\n",
    "        \"\"\"\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            loss = self.loss(target, inputs)\n",
    "\n",
    "        for weight in self.weights:\n",
    "            self.logs[\"weight_gradients\"][epoch][mini_batch] = weight.get_gradient(loss, tape).numpy()\n",
    "            weight.gradientDescent_update(loss, tape)\n",
    "\n",
    "        for layer in self.layers[1:]:\n",
    "            self.logs[\"bias_gradients\"][epoch][mini_batch] = weight.get_gradient(loss, tape).numpy()\n",
    "            layer.gradientDescent_update(loss, tape)\n",
    "    \n",
    "    def train_epochs(self, y_train, x_train, y_test, x_test, show=False):\n",
    "        \"\"\"\n",
    "        Perform multiple epochs of training based on global parameter\n",
    "        values.\n",
    "        \"\"\"\n",
    "        # Define the shapes for each log entry\n",
    "        log_shapes = {\n",
    "            \"train_losses\": (self.params['MAX_EPOCHS']+1, self.params['MINI_BATCH']),\n",
    "            \"train_corrects\": (self.params['MAX_EPOCHS']+1, self.params['MINI_BATCH']),\n",
    "            \"test_losses\": (self.params['MAX_EPOCHS']+1,),\n",
    "            \"test_corrects\": (self.params['MAX_EPOCHS']+1,),\n",
    "            \"weights\": (self.params['MAX_EPOCHS']+1, self.params['MINI_BATCH'], self.params['N_INPUTS'], self.params['N_HIDDEN']),\n",
    "            \"weight_output\": (self.params['MAX_EPOCHS']+1, self.params['MINI_BATCH'], len(x_train), self.params['N_INPUTS']),\n",
    "            \"biases\": (self.params['MAX_EPOCHS']+1, self.params['MINI_BATCH'], self.params['N_INPUTS']),\n",
    "            \"layer_output\": (self.params['MAX_EPOCHS']+1, self.params['MINI_BATCH'], len(x_train), self.params['N_INPUTS']),\n",
    "            \"nn_output\": (self.params['MAX_EPOCHS']+1, self.params['MINI_BATCH'], len(x_train), self.params['N_OUTPUTS']),\n",
    "            \"mini_batch_before_time\": (self.params['MAX_EPOCHS']+1, self.params['MINI_BATCH']),\n",
    "            \"mini_batch_after_time\": (self.params['MAX_EPOCHS']+1, self.params['MINI_BATCH']),\n",
    "            \"mini_batch_total_time\": (self.params['MAX_EPOCHS']+1, self.params['MINI_BATCH']),\n",
    "            \"weight_gradients\": (self.params['MAX_EPOCHS']+1, self.params['MINI_BATCH'], self.params['N_INPUTS'], self.params['N_HIDDEN']),\n",
    "            \"bias_gradients\": (self.params['MAX_EPOCHS']+1, self.params['MINI_BATCH'], self.params['N_HIDDEN'], self.params['N_OUTPUTS'])\n",
    "        }\n",
    "\n",
    "        # Initialize the logs with zeros\n",
    "        self.logs = {key: np.zeros(shape, dtype=np.float32) for key, shape in log_shapes.items()}\n",
    "\n",
    "        MB_SIZE = self.params['TRAINING_SIZE'] // self.params['MINI_BATCH']\n",
    "        self.logs['before_time'] = time.time()\n",
    "        mini_batch = 0\n",
    "\n",
    "        for epoch in range(self.params['MAX_EPOCHS']+1):\n",
    "            self.logs['test_losses'][epoch] = self.loss(y_test, x_test)\n",
    "            test_conf = self.confusion(y_test, x_test)\n",
    "            self.logs['test_corrects'][epoch] = tf.linalg.trace(test_conf).numpy()\n",
    "\n",
    "            x, y = double_shuffle(x_train, y_train)\n",
    "\n",
    "            for mini_batch in range(self.params['MINI_BATCH']):\n",
    "                self.logs['mini_batch_before_time'][epoch][mini_batch] = time.time()\n",
    "\n",
    "                mbx = x[MB_SIZE*mini_batch:MB_SIZE*(mini_batch+1)]\n",
    "                mby = y[MB_SIZE*mini_batch:MB_SIZE*(mini_batch+1)]\n",
    "\n",
    "                unshuffled_x = x_train[MB_SIZE*mini_batch:MB_SIZE*(mini_batch+1)]\n",
    "                unshuffled_y = y_train[MB_SIZE*mini_batch:MB_SIZE*(mini_batch+1)]\n",
    "\n",
    "                train_loss = self.loss(mby, mbx)\n",
    "                self.logs['train_losses'][epoch][mini_batch] = train_loss\n",
    "\n",
    "                conf = self.confusion(mby, mbx)\n",
    "                self.logs['train_corrects'][epoch][mini_batch] = tf.linalg.trace(conf)\n",
    "\n",
    "                \n",
    "\n",
    "                for idx, (weight, layer) in enumerate(zip(self.weights, self.layers[1:])):\n",
    "                    #print( weight.weights.name )\n",
    "                    \n",
    "                    self.logs['weights'][epoch][mini_batch] = weight.weights.numpy()\n",
    "                    self.logs['weight_output'][epoch][mini_batch] = weight.output(unshuffled_x).numpy()\n",
    "\n",
    "                    self.logs['biases'][epoch][mini_batch] = layer.biases.numpy()\n",
    "                    self.logs['layer_output'][epoch][mini_batch] = layer.output(unshuffled_x).numpy()\n",
    "\n",
    "                  \n",
    "\n",
    "\n",
    "                self.logs['nn_output'][epoch][mini_batch] = self.output(unshuffled_x).numpy()\n",
    "\n",
    "                if epoch != self.params['MAX_EPOCHS']:\n",
    "                    self.train(mby, mbx, epoch, mini_batch)\n",
    "\n",
    "                self.logs['mini_batch_after_time'][epoch][mini_batch] = time.time()\n",
    "                self.logs['mini_batch_total_time'][epoch][mini_batch] = self.logs['mini_batch_after_time'][epoch][mini_batch] - self.logs['mini_batch_before_time'][epoch][mini_batch]\n",
    "\n",
    "            if show:\n",
    "                sys.stdout.write(\"%d,    Training Loss: %lf,    Testing Loss: %lf,\\n\" % (epoch, self.loss(y_train, x_train), self.loss(y_test, x_test)))\n",
    "\n",
    "        self.logs['after_time'] = time.time()\n",
    "        self.logs['test_conf'] = test_conf\n",
    "        self.logs['total_time'] = self.logs['after_time'] - self.logs['before_time']\n",
    "\n",
    "        return self.logs\n",
    "\n",
    "@tf.function\n",
    "def preprocess(input):\n",
    "    \"\"\"\n",
    "    Converts the 28x28 pixel images into 784 dimensional feature vectors.\n",
    "    Also converts the uint8 into DTYPE.\n",
    "    \"\"\"\n",
    "    return tf.cast(tf.reshape(input, (input.shape[0], -1)), DTYPE) / 256.0\n",
    "\n",
    "@tf.function\n",
    "def one_hot(y_train):\n",
    "    \"\"\"\n",
    "    Converts labels into one-hot vectors.\n",
    "    \"\"\"\n",
    "    return tf.one_hot(y_train, 2)\n",
    "\n",
    "@tf.function\n",
    "def un_hot(inputs):\n",
    "    \"\"\"\n",
    "    Converts one-hot vectors back into labels.\n",
    "    \"\"\"\n",
    "    return tf.math.argmax(inputs, axis=1)\n",
    "\n",
    "def double_shuffle(inputs, targets):\n",
    "    \"\"\"\n",
    "    Shuffles the inputs and targets in the same order.\n",
    "    \"\"\"\n",
    "    if inputs.shape[0] != targets.shape[0]:\n",
    "        raise ValueError(\"Inputs and targets must have the same first dimension!\")\n",
    "\n",
    "    shuffle_indices = tf.random.shuffle(tf.range(inputs.shape[0]))\n",
    "    return tf.gather(inputs, shuffle_indices).numpy(), tf.gather(targets, shuffle_indices).numpy()\n",
    "\n",
    "def gpu_utilization():\n",
    "    \"\"\"\n",
    "    Returns the GPU utilization rates.\n",
    "    \"\"\"\n",
    "    return [nv.nvmlDeviceGetUtilizationRates(handle).gpu for handle in handles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_hidden_output <__main__.WeightLayer object at 0x7fd97c780250>\n",
      "W_input_hidden <__main__.WeightLayer object at 0x7fd92b952610>\n",
      "W_input_hidden input hidden (4, 2) (2, 2)\n",
      "W_hidden_output hidden output (4, 2) (2, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,    Training Loss: 0.693021,    Testing Loss: 0.693021,\n",
      "1,    Training Loss: 0.692913,    Testing Loss: 0.692913,\n",
      "2,    Training Loss: 0.692827,    Testing Loss: 0.692827,\n",
      "3,    Training Loss: 0.692754,    Testing Loss: 0.692754,\n",
      "4,    Training Loss: 0.692689,    Testing Loss: 0.692689,\n",
      "5,    Training Loss: 0.692630,    Testing Loss: 0.692630,\n",
      "6,    Training Loss: 0.692574,    Testing Loss: 0.692574,\n",
      "7,    Training Loss: 0.692519,    Testing Loss: 0.692519,\n",
      "8,    Training Loss: 0.692467,    Testing Loss: 0.692467,\n",
      "9,    Training Loss: 0.692415,    Testing Loss: 0.692415,\n",
      "10,    Training Loss: 0.692363,    Testing Loss: 0.692363,\n",
      "11,    Training Loss: 0.692312,    Testing Loss: 0.692312,\n",
      "12,    Training Loss: 0.692262,    Testing Loss: 0.692262,\n",
      "13,    Training Loss: 0.692211,    Testing Loss: 0.692211,\n",
      "14,    Training Loss: 0.692161,    Testing Loss: 0.692161,\n",
      "15,    Training Loss: 0.692111,    Testing Loss: 0.692111,\n",
      "16,    Training Loss: 0.692061,    Testing Loss: 0.692061,\n",
      "17,    Training Loss: 0.692011,    Testing Loss: 0.692011,\n",
      "18,    Training Loss: 0.691961,    Testing Loss: 0.691961,\n",
      "19,    Training Loss: 0.691912,    Testing Loss: 0.691912,\n",
      "20,    Training Loss: 0.691862,    Testing Loss: 0.691862,\n",
      "21,    Training Loss: 0.691813,    Testing Loss: 0.691813,\n",
      "22,    Training Loss: 0.691763,    Testing Loss: 0.691763,\n",
      "23,    Training Loss: 0.691714,    Testing Loss: 0.691714,\n",
      "24,    Training Loss: 0.691664,    Testing Loss: 0.691664,\n",
      "25,    Training Loss: 0.691615,    Testing Loss: 0.691615,\n",
      "26,    Training Loss: 0.691565,    Testing Loss: 0.691565,\n",
      "27,    Training Loss: 0.691516,    Testing Loss: 0.691516,\n",
      "28,    Training Loss: 0.691466,    Testing Loss: 0.691466,\n",
      "29,    Training Loss: 0.691417,    Testing Loss: 0.691417,\n",
      "30,    Training Loss: 0.691367,    Testing Loss: 0.691367,\n",
      "31,    Training Loss: 0.691317,    Testing Loss: 0.691317,\n",
      "32,    Training Loss: 0.691268,    Testing Loss: 0.691268,\n",
      "33,    Training Loss: 0.691218,    Testing Loss: 0.691218,\n",
      "34,    Training Loss: 0.691168,    Testing Loss: 0.691168,\n",
      "35,    Training Loss: 0.691118,    Testing Loss: 0.691118,\n",
      "36,    Training Loss: 0.691068,    Testing Loss: 0.691068,\n",
      "37,    Training Loss: 0.691018,    Testing Loss: 0.691018,\n",
      "38,    Training Loss: 0.690968,    Testing Loss: 0.690968,\n",
      "39,    Training Loss: 0.690917,    Testing Loss: 0.690917,\n",
      "40,    Training Loss: 0.690866,    Testing Loss: 0.690866,\n",
      "41,    Training Loss: 0.690816,    Testing Loss: 0.690816,\n",
      "42,    Training Loss: 0.690765,    Testing Loss: 0.690765,\n",
      "43,    Training Loss: 0.690714,    Testing Loss: 0.690714,\n",
      "44,    Training Loss: 0.690663,    Testing Loss: 0.690663,\n",
      "45,    Training Loss: 0.690611,    Testing Loss: 0.690611,\n",
      "46,    Training Loss: 0.690559,    Testing Loss: 0.690559,\n",
      "47,    Training Loss: 0.690507,    Testing Loss: 0.690507,\n",
      "48,    Training Loss: 0.690455,    Testing Loss: 0.690455,\n",
      "49,    Training Loss: 0.690403,    Testing Loss: 0.690403,\n",
      "50,    Training Loss: 0.690350,    Testing Loss: 0.690350,\n",
      "51,    Training Loss: 0.690297,    Testing Loss: 0.690297,\n",
      "52,    Training Loss: 0.690244,    Testing Loss: 0.690244,\n",
      "53,    Training Loss: 0.690191,    Testing Loss: 0.690191,\n",
      "54,    Training Loss: 0.690137,    Testing Loss: 0.690137,\n",
      "55,    Training Loss: 0.690083,    Testing Loss: 0.690083,\n",
      "56,    Training Loss: 0.690029,    Testing Loss: 0.690029,\n",
      "57,    Training Loss: 0.689974,    Testing Loss: 0.689974,\n",
      "58,    Training Loss: 0.689919,    Testing Loss: 0.689919,\n",
      "59,    Training Loss: 0.689864,    Testing Loss: 0.689864,\n",
      "60,    Training Loss: 0.689808,    Testing Loss: 0.689808,\n",
      "61,    Training Loss: 0.689752,    Testing Loss: 0.689752,\n",
      "62,    Training Loss: 0.689696,    Testing Loss: 0.689696,\n",
      "63,    Training Loss: 0.689639,    Testing Loss: 0.689639,\n",
      "64,    Training Loss: 0.689582,    Testing Loss: 0.689582,\n",
      "65,    Training Loss: 0.689524,    Testing Loss: 0.689524,\n",
      "66,    Training Loss: 0.689466,    Testing Loss: 0.689466,\n",
      "67,    Training Loss: 0.689407,    Testing Loss: 0.689407,\n",
      "68,    Training Loss: 0.689348,    Testing Loss: 0.689348,\n",
      "69,    Training Loss: 0.689289,    Testing Loss: 0.689289,\n",
      "70,    Training Loss: 0.689229,    Testing Loss: 0.689229,\n",
      "71,    Training Loss: 0.689169,    Testing Loss: 0.689169,\n",
      "72,    Training Loss: 0.689108,    Testing Loss: 0.689108,\n",
      "73,    Training Loss: 0.689047,    Testing Loss: 0.689047,\n",
      "74,    Training Loss: 0.688985,    Testing Loss: 0.688985,\n",
      "75,    Training Loss: 0.688922,    Testing Loss: 0.688922,\n",
      "76,    Training Loss: 0.688860,    Testing Loss: 0.688860,\n",
      "77,    Training Loss: 0.688796,    Testing Loss: 0.688796,\n",
      "78,    Training Loss: 0.688732,    Testing Loss: 0.688732,\n",
      "79,    Training Loss: 0.688667,    Testing Loss: 0.688667,\n",
      "80,    Training Loss: 0.688602,    Testing Loss: 0.688602,\n",
      "81,    Training Loss: 0.688536,    Testing Loss: 0.688536,\n",
      "82,    Training Loss: 0.688470,    Testing Loss: 0.688470,\n",
      "83,    Training Loss: 0.688403,    Testing Loss: 0.688403,\n",
      "84,    Training Loss: 0.688335,    Testing Loss: 0.688335,\n",
      "85,    Training Loss: 0.688267,    Testing Loss: 0.688267,\n",
      "86,    Training Loss: 0.688198,    Testing Loss: 0.688198,\n",
      "87,    Training Loss: 0.688128,    Testing Loss: 0.688128,\n",
      "88,    Training Loss: 0.688058,    Testing Loss: 0.688058,\n",
      "89,    Training Loss: 0.687987,    Testing Loss: 0.687987,\n",
      "90,    Training Loss: 0.687915,    Testing Loss: 0.687915,\n",
      "91,    Training Loss: 0.687842,    Testing Loss: 0.687842,\n",
      "92,    Training Loss: 0.687769,    Testing Loss: 0.687769,\n",
      "93,    Training Loss: 0.687695,    Testing Loss: 0.687695,\n",
      "94,    Training Loss: 0.687620,    Testing Loss: 0.687620,\n",
      "95,    Training Loss: 0.687544,    Testing Loss: 0.687544,\n",
      "96,    Training Loss: 0.687468,    Testing Loss: 0.687468,\n",
      "97,    Training Loss: 0.687391,    Testing Loss: 0.687391,\n",
      "98,    Training Loss: 0.687312,    Testing Loss: 0.687312,\n",
      "99,    Training Loss: 0.687234,    Testing Loss: 0.687234,\n",
      "100,    Training Loss: 0.687154,    Testing Loss: 0.687154,\n",
      "101,    Training Loss: 0.687073,    Testing Loss: 0.687073,\n",
      "102,    Training Loss: 0.686991,    Testing Loss: 0.686991,\n",
      "103,    Training Loss: 0.686909,    Testing Loss: 0.686909,\n",
      "104,    Training Loss: 0.686826,    Testing Loss: 0.686826,\n",
      "105,    Training Loss: 0.686741,    Testing Loss: 0.686741,\n",
      "106,    Training Loss: 0.686656,    Testing Loss: 0.686656,\n",
      "107,    Training Loss: 0.686569,    Testing Loss: 0.686569,\n",
      "108,    Training Loss: 0.686482,    Testing Loss: 0.686482,\n",
      "109,    Training Loss: 0.686394,    Testing Loss: 0.686394,\n",
      "110,    Training Loss: 0.686305,    Testing Loss: 0.686305,\n",
      "111,    Training Loss: 0.686214,    Testing Loss: 0.686214,\n",
      "112,    Training Loss: 0.686123,    Testing Loss: 0.686123,\n",
      "113,    Training Loss: 0.686030,    Testing Loss: 0.686030,\n",
      "114,    Training Loss: 0.685937,    Testing Loss: 0.685937,\n",
      "115,    Training Loss: 0.685842,    Testing Loss: 0.685842,\n",
      "116,    Training Loss: 0.685746,    Testing Loss: 0.685746,\n",
      "117,    Training Loss: 0.685649,    Testing Loss: 0.685649,\n",
      "118,    Training Loss: 0.685551,    Testing Loss: 0.685551,\n",
      "119,    Training Loss: 0.685452,    Testing Loss: 0.685452,\n",
      "120,    Training Loss: 0.685352,    Testing Loss: 0.685352,\n",
      "121,    Training Loss: 0.685250,    Testing Loss: 0.685250,\n",
      "122,    Training Loss: 0.685147,    Testing Loss: 0.685147,\n",
      "123,    Training Loss: 0.685043,    Testing Loss: 0.685043,\n",
      "124,    Training Loss: 0.684937,    Testing Loss: 0.684937,\n",
      "125,    Training Loss: 0.684831,    Testing Loss: 0.684831,\n",
      "126,    Training Loss: 0.684723,    Testing Loss: 0.684723,\n",
      "127,    Training Loss: 0.684613,    Testing Loss: 0.684613,\n",
      "128,    Training Loss: 0.684503,    Testing Loss: 0.684503,\n",
      "129,    Training Loss: 0.684391,    Testing Loss: 0.684391,\n",
      "130,    Training Loss: 0.684277,    Testing Loss: 0.684277,\n",
      "131,    Training Loss: 0.684162,    Testing Loss: 0.684162,\n",
      "132,    Training Loss: 0.684046,    Testing Loss: 0.684046,\n",
      "133,    Training Loss: 0.683929,    Testing Loss: 0.683929,\n",
      "134,    Training Loss: 0.683810,    Testing Loss: 0.683810,\n",
      "135,    Training Loss: 0.683689,    Testing Loss: 0.683689,\n",
      "136,    Training Loss: 0.683567,    Testing Loss: 0.683567,\n",
      "137,    Training Loss: 0.683443,    Testing Loss: 0.683443,\n",
      "138,    Training Loss: 0.683318,    Testing Loss: 0.683318,\n",
      "139,    Training Loss: 0.683192,    Testing Loss: 0.683192,\n",
      "140,    Training Loss: 0.683063,    Testing Loss: 0.683063,\n",
      "141,    Training Loss: 0.682933,    Testing Loss: 0.682933,\n",
      "142,    Training Loss: 0.682802,    Testing Loss: 0.682802,\n",
      "143,    Training Loss: 0.682669,    Testing Loss: 0.682669,\n",
      "144,    Training Loss: 0.682534,    Testing Loss: 0.682534,\n",
      "145,    Training Loss: 0.682398,    Testing Loss: 0.682398,\n",
      "146,    Training Loss: 0.682259,    Testing Loss: 0.682259,\n",
      "147,    Training Loss: 0.682119,    Testing Loss: 0.682119,\n",
      "148,    Training Loss: 0.681978,    Testing Loss: 0.681978,\n",
      "149,    Training Loss: 0.681834,    Testing Loss: 0.681834,\n",
      "150,    Training Loss: 0.681689,    Testing Loss: 0.681689,\n",
      "151,    Training Loss: 0.681542,    Testing Loss: 0.681542,\n",
      "152,    Training Loss: 0.681393,    Testing Loss: 0.681393,\n",
      "153,    Training Loss: 0.681242,    Testing Loss: 0.681242,\n",
      "154,    Training Loss: 0.681090,    Testing Loss: 0.681090,\n",
      "155,    Training Loss: 0.680935,    Testing Loss: 0.680935,\n",
      "156,    Training Loss: 0.680779,    Testing Loss: 0.680779,\n",
      "157,    Training Loss: 0.680620,    Testing Loss: 0.680620,\n",
      "158,    Training Loss: 0.680460,    Testing Loss: 0.680460,\n",
      "159,    Training Loss: 0.680297,    Testing Loss: 0.680297,\n",
      "160,    Training Loss: 0.680133,    Testing Loss: 0.680133,\n",
      "161,    Training Loss: 0.679967,    Testing Loss: 0.679967,\n",
      "162,    Training Loss: 0.679798,    Testing Loss: 0.679798,\n",
      "163,    Training Loss: 0.679627,    Testing Loss: 0.679627,\n",
      "164,    Training Loss: 0.679455,    Testing Loss: 0.679455,\n",
      "165,    Training Loss: 0.679280,    Testing Loss: 0.679280,\n",
      "166,    Training Loss: 0.679103,    Testing Loss: 0.679103,\n",
      "167,    Training Loss: 0.678923,    Testing Loss: 0.678923,\n",
      "168,    Training Loss: 0.678742,    Testing Loss: 0.678742,\n",
      "169,    Training Loss: 0.678558,    Testing Loss: 0.678558,\n",
      "170,    Training Loss: 0.678372,    Testing Loss: 0.678372,\n",
      "171,    Training Loss: 0.678184,    Testing Loss: 0.678184,\n",
      "172,    Training Loss: 0.677993,    Testing Loss: 0.677993,\n",
      "173,    Training Loss: 0.677800,    Testing Loss: 0.677800,\n",
      "174,    Training Loss: 0.677604,    Testing Loss: 0.677604,\n",
      "175,    Training Loss: 0.677407,    Testing Loss: 0.677407,\n",
      "176,    Training Loss: 0.677206,    Testing Loss: 0.677206,\n",
      "177,    Training Loss: 0.677004,    Testing Loss: 0.677004,\n",
      "178,    Training Loss: 0.676798,    Testing Loss: 0.676798,\n",
      "179,    Training Loss: 0.676591,    Testing Loss: 0.676591,\n",
      "180,    Training Loss: 0.676381,    Testing Loss: 0.676381,\n",
      "181,    Training Loss: 0.676168,    Testing Loss: 0.676168,\n",
      "182,    Training Loss: 0.675952,    Testing Loss: 0.675952,\n",
      "183,    Training Loss: 0.675735,    Testing Loss: 0.675735,\n",
      "184,    Training Loss: 0.675514,    Testing Loss: 0.675514,\n",
      "185,    Training Loss: 0.675291,    Testing Loss: 0.675291,\n",
      "186,    Training Loss: 0.675065,    Testing Loss: 0.675065,\n",
      "187,    Training Loss: 0.674836,    Testing Loss: 0.674836,\n",
      "188,    Training Loss: 0.674605,    Testing Loss: 0.674605,\n",
      "189,    Training Loss: 0.674370,    Testing Loss: 0.674370,\n",
      "190,    Training Loss: 0.674133,    Testing Loss: 0.674133,\n",
      "191,    Training Loss: 0.673893,    Testing Loss: 0.673893,\n",
      "192,    Training Loss: 0.673651,    Testing Loss: 0.673651,\n",
      "193,    Training Loss: 0.673405,    Testing Loss: 0.673405,\n",
      "194,    Training Loss: 0.673156,    Testing Loss: 0.673156,\n",
      "195,    Training Loss: 0.672905,    Testing Loss: 0.672905,\n",
      "196,    Training Loss: 0.672650,    Testing Loss: 0.672650,\n",
      "197,    Training Loss: 0.672393,    Testing Loss: 0.672393,\n",
      "198,    Training Loss: 0.672132,    Testing Loss: 0.672132,\n",
      "199,    Training Loss: 0.671869,    Testing Loss: 0.671869,\n",
      "200,    Training Loss: 0.671602,    Testing Loss: 0.671602,\n",
      "201,    Training Loss: 0.671332,    Testing Loss: 0.671332,\n",
      "202,    Training Loss: 0.671059,    Testing Loss: 0.671059,\n",
      "203,    Training Loss: 0.670783,    Testing Loss: 0.670783,\n",
      "204,    Training Loss: 0.670503,    Testing Loss: 0.670503,\n",
      "205,    Training Loss: 0.670221,    Testing Loss: 0.670221,\n",
      "206,    Training Loss: 0.669935,    Testing Loss: 0.669935,\n",
      "207,    Training Loss: 0.669645,    Testing Loss: 0.669645,\n",
      "208,    Training Loss: 0.669353,    Testing Loss: 0.669353,\n",
      "209,    Training Loss: 0.669057,    Testing Loss: 0.669057,\n",
      "210,    Training Loss: 0.668757,    Testing Loss: 0.668757,\n",
      "211,    Training Loss: 0.668454,    Testing Loss: 0.668454,\n",
      "212,    Training Loss: 0.668148,    Testing Loss: 0.668148,\n",
      "213,    Training Loss: 0.667838,    Testing Loss: 0.667838,\n",
      "214,    Training Loss: 0.667525,    Testing Loss: 0.667525,\n",
      "215,    Training Loss: 0.667208,    Testing Loss: 0.667208,\n",
      "216,    Training Loss: 0.666888,    Testing Loss: 0.666888,\n",
      "217,    Training Loss: 0.666564,    Testing Loss: 0.666564,\n",
      "218,    Training Loss: 0.666236,    Testing Loss: 0.666236,\n",
      "219,    Training Loss: 0.665904,    Testing Loss: 0.665904,\n",
      "220,    Training Loss: 0.665569,    Testing Loss: 0.665569,\n",
      "221,    Training Loss: 0.665230,    Testing Loss: 0.665230,\n",
      "222,    Training Loss: 0.664888,    Testing Loss: 0.664888,\n",
      "223,    Training Loss: 0.664541,    Testing Loss: 0.664541,\n",
      "224,    Training Loss: 0.664191,    Testing Loss: 0.664191,\n",
      "225,    Training Loss: 0.663836,    Testing Loss: 0.663836,\n",
      "226,    Training Loss: 0.663478,    Testing Loss: 0.663478,\n",
      "227,    Training Loss: 0.663116,    Testing Loss: 0.663116,\n",
      "228,    Training Loss: 0.662750,    Testing Loss: 0.662750,\n",
      "229,    Training Loss: 0.662380,    Testing Loss: 0.662380,\n",
      "230,    Training Loss: 0.662006,    Testing Loss: 0.662006,\n",
      "231,    Training Loss: 0.661628,    Testing Loss: 0.661628,\n",
      "232,    Training Loss: 0.661246,    Testing Loss: 0.661246,\n",
      "233,    Training Loss: 0.660859,    Testing Loss: 0.660859,\n",
      "234,    Training Loss: 0.660469,    Testing Loss: 0.660469,\n",
      "235,    Training Loss: 0.660074,    Testing Loss: 0.660074,\n",
      "236,    Training Loss: 0.659675,    Testing Loss: 0.659675,\n",
      "237,    Training Loss: 0.659272,    Testing Loss: 0.659272,\n",
      "238,    Training Loss: 0.658864,    Testing Loss: 0.658864,\n",
      "239,    Training Loss: 0.658452,    Testing Loss: 0.658452,\n",
      "240,    Training Loss: 0.658036,    Testing Loss: 0.658036,\n",
      "241,    Training Loss: 0.657615,    Testing Loss: 0.657615,\n",
      "242,    Training Loss: 0.657190,    Testing Loss: 0.657190,\n",
      "243,    Training Loss: 0.656761,    Testing Loss: 0.656761,\n",
      "244,    Training Loss: 0.656327,    Testing Loss: 0.656327,\n",
      "245,    Training Loss: 0.655888,    Testing Loss: 0.655888,\n",
      "246,    Training Loss: 0.655445,    Testing Loss: 0.655445,\n",
      "247,    Training Loss: 0.654998,    Testing Loss: 0.654998,\n",
      "248,    Training Loss: 0.654545,    Testing Loss: 0.654545,\n",
      "249,    Training Loss: 0.654089,    Testing Loss: 0.654089,\n",
      "250,    Training Loss: 0.653627,    Testing Loss: 0.653627,\n",
      "251,    Training Loss: 0.653161,    Testing Loss: 0.653161,\n",
      "252,    Training Loss: 0.652690,    Testing Loss: 0.652690,\n",
      "253,    Training Loss: 0.652214,    Testing Loss: 0.652214,\n",
      "254,    Training Loss: 0.651734,    Testing Loss: 0.651734,\n",
      "255,    Training Loss: 0.651249,    Testing Loss: 0.651249,\n",
      "256,    Training Loss: 0.650759,    Testing Loss: 0.650759,\n",
      "257,    Training Loss: 0.650264,    Testing Loss: 0.650264,\n",
      "258,    Training Loss: 0.649764,    Testing Loss: 0.649764,\n",
      "259,    Training Loss: 0.649259,    Testing Loss: 0.649259,\n",
      "260,    Training Loss: 0.648749,    Testing Loss: 0.648749,\n",
      "261,    Training Loss: 0.648235,    Testing Loss: 0.648235,\n",
      "262,    Training Loss: 0.647715,    Testing Loss: 0.647715,\n",
      "263,    Training Loss: 0.647191,    Testing Loss: 0.647191,\n",
      "264,    Training Loss: 0.646661,    Testing Loss: 0.646661,\n",
      "265,    Training Loss: 0.646126,    Testing Loss: 0.646126,\n",
      "266,    Training Loss: 0.645586,    Testing Loss: 0.645586,\n",
      "267,    Training Loss: 0.645042,    Testing Loss: 0.645042,\n",
      "268,    Training Loss: 0.644492,    Testing Loss: 0.644492,\n",
      "269,    Training Loss: 0.643937,    Testing Loss: 0.643937,\n",
      "270,    Training Loss: 0.643376,    Testing Loss: 0.643376,\n",
      "271,    Training Loss: 0.642811,    Testing Loss: 0.642811,\n",
      "272,    Training Loss: 0.642240,    Testing Loss: 0.642240,\n",
      "273,    Training Loss: 0.641664,    Testing Loss: 0.641664,\n",
      "274,    Training Loss: 0.641083,    Testing Loss: 0.641083,\n",
      "275,    Training Loss: 0.640497,    Testing Loss: 0.640497,\n",
      "276,    Training Loss: 0.639905,    Testing Loss: 0.639905,\n",
      "277,    Training Loss: 0.639309,    Testing Loss: 0.639309,\n",
      "278,    Training Loss: 0.638706,    Testing Loss: 0.638706,\n",
      "279,    Training Loss: 0.638099,    Testing Loss: 0.638099,\n",
      "280,    Training Loss: 0.637487,    Testing Loss: 0.637487,\n",
      "281,    Training Loss: 0.636868,    Testing Loss: 0.636868,\n",
      "282,    Training Loss: 0.636245,    Testing Loss: 0.636245,\n",
      "283,    Training Loss: 0.635617,    Testing Loss: 0.635617,\n",
      "284,    Training Loss: 0.634983,    Testing Loss: 0.634983,\n",
      "285,    Training Loss: 0.634344,    Testing Loss: 0.634344,\n",
      "286,    Training Loss: 0.633699,    Testing Loss: 0.633699,\n",
      "287,    Training Loss: 0.633049,    Testing Loss: 0.633049,\n",
      "288,    Training Loss: 0.632394,    Testing Loss: 0.632394,\n",
      "289,    Training Loss: 0.631733,    Testing Loss: 0.631733,\n",
      "290,    Training Loss: 0.631067,    Testing Loss: 0.631067,\n",
      "291,    Training Loss: 0.630396,    Testing Loss: 0.630396,\n",
      "292,    Training Loss: 0.629720,    Testing Loss: 0.629720,\n",
      "293,    Training Loss: 0.629038,    Testing Loss: 0.629038,\n",
      "294,    Training Loss: 0.628351,    Testing Loss: 0.628351,\n",
      "295,    Training Loss: 0.627658,    Testing Loss: 0.627658,\n",
      "296,    Training Loss: 0.626961,    Testing Loss: 0.626961,\n",
      "297,    Training Loss: 0.626258,    Testing Loss: 0.626258,\n",
      "298,    Training Loss: 0.625550,    Testing Loss: 0.625550,\n",
      "299,    Training Loss: 0.624836,    Testing Loss: 0.624836,\n",
      "300,    Training Loss: 0.624117,    Testing Loss: 0.624117,\n",
      "301,    Training Loss: 0.623394,    Testing Loss: 0.623394,\n",
      "302,    Training Loss: 0.622665,    Testing Loss: 0.622665,\n",
      "303,    Training Loss: 0.621930,    Testing Loss: 0.621930,\n",
      "304,    Training Loss: 0.621191,    Testing Loss: 0.621191,\n",
      "305,    Training Loss: 0.620447,    Testing Loss: 0.620447,\n",
      "306,    Training Loss: 0.619697,    Testing Loss: 0.619697,\n",
      "307,    Training Loss: 0.618942,    Testing Loss: 0.618942,\n",
      "308,    Training Loss: 0.618183,    Testing Loss: 0.618183,\n",
      "309,    Training Loss: 0.617418,    Testing Loss: 0.617418,\n",
      "310,    Training Loss: 0.616649,    Testing Loss: 0.616649,\n",
      "311,    Training Loss: 0.615874,    Testing Loss: 0.615874,\n",
      "312,    Training Loss: 0.615095,    Testing Loss: 0.615095,\n",
      "313,    Training Loss: 0.614311,    Testing Loss: 0.614311,\n",
      "314,    Training Loss: 0.613522,    Testing Loss: 0.613522,\n",
      "315,    Training Loss: 0.612728,    Testing Loss: 0.612728,\n",
      "316,    Training Loss: 0.611929,    Testing Loss: 0.611929,\n",
      "317,    Training Loss: 0.611126,    Testing Loss: 0.611126,\n",
      "318,    Training Loss: 0.610318,    Testing Loss: 0.610318,\n",
      "319,    Training Loss: 0.609506,    Testing Loss: 0.609506,\n",
      "320,    Training Loss: 0.608689,    Testing Loss: 0.608689,\n",
      "321,    Training Loss: 0.607867,    Testing Loss: 0.607867,\n",
      "322,    Training Loss: 0.607042,    Testing Loss: 0.607042,\n",
      "323,    Training Loss: 0.606211,    Testing Loss: 0.606211,\n",
      "324,    Training Loss: 0.605377,    Testing Loss: 0.605377,\n",
      "325,    Training Loss: 0.604538,    Testing Loss: 0.604538,\n",
      "326,    Training Loss: 0.603695,    Testing Loss: 0.603695,\n",
      "327,    Training Loss: 0.602848,    Testing Loss: 0.602848,\n",
      "328,    Training Loss: 0.601997,    Testing Loss: 0.601997,\n",
      "329,    Training Loss: 0.601142,    Testing Loss: 0.601142,\n",
      "330,    Training Loss: 0.600282,    Testing Loss: 0.600282,\n",
      "331,    Training Loss: 0.599419,    Testing Loss: 0.599419,\n",
      "332,    Training Loss: 0.598553,    Testing Loss: 0.598553,\n",
      "333,    Training Loss: 0.597682,    Testing Loss: 0.597682,\n",
      "334,    Training Loss: 0.596808,    Testing Loss: 0.596808,\n",
      "335,    Training Loss: 0.595930,    Testing Loss: 0.595930,\n",
      "336,    Training Loss: 0.595049,    Testing Loss: 0.595049,\n",
      "337,    Training Loss: 0.594164,    Testing Loss: 0.594164,\n",
      "338,    Training Loss: 0.593276,    Testing Loss: 0.593276,\n",
      "339,    Training Loss: 0.592385,    Testing Loss: 0.592385,\n",
      "340,    Training Loss: 0.591490,    Testing Loss: 0.591490,\n",
      "341,    Training Loss: 0.590592,    Testing Loss: 0.590592,\n",
      "342,    Training Loss: 0.589692,    Testing Loss: 0.589692,\n",
      "343,    Training Loss: 0.588788,    Testing Loss: 0.588788,\n",
      "344,    Training Loss: 0.587881,    Testing Loss: 0.587881,\n",
      "345,    Training Loss: 0.586972,    Testing Loss: 0.586972,\n",
      "346,    Training Loss: 0.586060,    Testing Loss: 0.586060,\n",
      "347,    Training Loss: 0.585145,    Testing Loss: 0.585145,\n",
      "348,    Training Loss: 0.584228,    Testing Loss: 0.584228,\n",
      "349,    Training Loss: 0.583308,    Testing Loss: 0.583308,\n",
      "350,    Training Loss: 0.582386,    Testing Loss: 0.582386,\n",
      "351,    Training Loss: 0.581462,    Testing Loss: 0.581462,\n",
      "352,    Training Loss: 0.580536,    Testing Loss: 0.580536,\n",
      "353,    Training Loss: 0.579607,    Testing Loss: 0.579607,\n",
      "354,    Training Loss: 0.578677,    Testing Loss: 0.578677,\n",
      "355,    Training Loss: 0.577744,    Testing Loss: 0.577744,\n",
      "356,    Training Loss: 0.576810,    Testing Loss: 0.576810,\n",
      "357,    Training Loss: 0.575874,    Testing Loss: 0.575874,\n",
      "358,    Training Loss: 0.574937,    Testing Loss: 0.574937,\n",
      "359,    Training Loss: 0.573998,    Testing Loss: 0.573998,\n",
      "360,    Training Loss: 0.573058,    Testing Loss: 0.573058,\n",
      "361,    Training Loss: 0.572116,    Testing Loss: 0.572116,\n",
      "362,    Training Loss: 0.571173,    Testing Loss: 0.571173,\n",
      "363,    Training Loss: 0.570229,    Testing Loss: 0.570229,\n",
      "364,    Training Loss: 0.569283,    Testing Loss: 0.569283,\n",
      "365,    Training Loss: 0.568337,    Testing Loss: 0.568337,\n",
      "366,    Training Loss: 0.567390,    Testing Loss: 0.567390,\n",
      "367,    Training Loss: 0.566442,    Testing Loss: 0.566442,\n",
      "368,    Training Loss: 0.565494,    Testing Loss: 0.565494,\n",
      "369,    Training Loss: 0.564545,    Testing Loss: 0.564545,\n",
      "370,    Training Loss: 0.563596,    Testing Loss: 0.563596,\n",
      "371,    Training Loss: 0.562646,    Testing Loss: 0.562646,\n",
      "372,    Training Loss: 0.561696,    Testing Loss: 0.561696,\n",
      "373,    Training Loss: 0.560745,    Testing Loss: 0.560745,\n",
      "374,    Training Loss: 0.559795,    Testing Loss: 0.559795,\n",
      "375,    Training Loss: 0.558844,    Testing Loss: 0.558844,\n",
      "376,    Training Loss: 0.557894,    Testing Loss: 0.557894,\n",
      "377,    Training Loss: 0.556944,    Testing Loss: 0.556944,\n",
      "378,    Training Loss: 0.555994,    Testing Loss: 0.555994,\n",
      "379,    Training Loss: 0.555044,    Testing Loss: 0.555044,\n",
      "380,    Training Loss: 0.554095,    Testing Loss: 0.554095,\n",
      "381,    Training Loss: 0.553146,    Testing Loss: 0.553146,\n",
      "382,    Training Loss: 0.552198,    Testing Loss: 0.552198,\n",
      "383,    Training Loss: 0.551251,    Testing Loss: 0.551251,\n",
      "384,    Training Loss: 0.550304,    Testing Loss: 0.550304,\n",
      "385,    Training Loss: 0.549359,    Testing Loss: 0.549359,\n",
      "386,    Training Loss: 0.548414,    Testing Loss: 0.548414,\n",
      "387,    Training Loss: 0.547471,    Testing Loss: 0.547471,\n",
      "388,    Training Loss: 0.546528,    Testing Loss: 0.546528,\n",
      "389,    Training Loss: 0.545587,    Testing Loss: 0.545587,\n",
      "390,    Training Loss: 0.544647,    Testing Loss: 0.544647,\n",
      "391,    Training Loss: 0.543708,    Testing Loss: 0.543708,\n",
      "392,    Training Loss: 0.542771,    Testing Loss: 0.542771,\n",
      "393,    Training Loss: 0.541836,    Testing Loss: 0.541836,\n",
      "394,    Training Loss: 0.540902,    Testing Loss: 0.540902,\n",
      "395,    Training Loss: 0.539969,    Testing Loss: 0.539969,\n",
      "396,    Training Loss: 0.539039,    Testing Loss: 0.539039,\n",
      "397,    Training Loss: 0.538110,    Testing Loss: 0.538110,\n",
      "398,    Training Loss: 0.537183,    Testing Loss: 0.537183,\n",
      "399,    Training Loss: 0.536259,    Testing Loss: 0.536259,\n",
      "400,    Training Loss: 0.535336,    Testing Loss: 0.535336,\n",
      "401,    Training Loss: 0.534415,    Testing Loss: 0.534415,\n",
      "402,    Training Loss: 0.533497,    Testing Loss: 0.533497,\n",
      "403,    Training Loss: 0.532581,    Testing Loss: 0.532581,\n",
      "404,    Training Loss: 0.531667,    Testing Loss: 0.531667,\n",
      "405,    Training Loss: 0.530756,    Testing Loss: 0.530756,\n",
      "406,    Training Loss: 0.529847,    Testing Loss: 0.529847,\n",
      "407,    Training Loss: 0.528940,    Testing Loss: 0.528940,\n",
      "408,    Training Loss: 0.528036,    Testing Loss: 0.528036,\n",
      "409,    Training Loss: 0.527135,    Testing Loss: 0.527135,\n",
      "410,    Training Loss: 0.526237,    Testing Loss: 0.526237,\n",
      "411,    Training Loss: 0.525341,    Testing Loss: 0.525341,\n",
      "412,    Training Loss: 0.524448,    Testing Loss: 0.524448,\n",
      "413,    Training Loss: 0.523558,    Testing Loss: 0.523558,\n",
      "414,    Training Loss: 0.522670,    Testing Loss: 0.522670,\n",
      "415,    Training Loss: 0.521786,    Testing Loss: 0.521786,\n",
      "416,    Training Loss: 0.520905,    Testing Loss: 0.520905,\n",
      "417,    Training Loss: 0.520027,    Testing Loss: 0.520027,\n",
      "418,    Training Loss: 0.519152,    Testing Loss: 0.519152,\n",
      "419,    Training Loss: 0.518280,    Testing Loss: 0.518280,\n",
      "420,    Training Loss: 0.517411,    Testing Loss: 0.517411,\n",
      "421,    Training Loss: 0.516546,    Testing Loss: 0.516546,\n",
      "422,    Training Loss: 0.515683,    Testing Loss: 0.515683,\n",
      "423,    Training Loss: 0.514825,    Testing Loss: 0.514825,\n",
      "424,    Training Loss: 0.513969,    Testing Loss: 0.513969,\n",
      "425,    Training Loss: 0.513117,    Testing Loss: 0.513117,\n",
      "426,    Training Loss: 0.512268,    Testing Loss: 0.512268,\n",
      "427,    Training Loss: 0.511423,    Testing Loss: 0.511423,\n",
      "428,    Training Loss: 0.510582,    Testing Loss: 0.510582,\n",
      "429,    Training Loss: 0.509744,    Testing Loss: 0.509744,\n",
      "430,    Training Loss: 0.508909,    Testing Loss: 0.508909,\n",
      "431,    Training Loss: 0.508078,    Testing Loss: 0.508078,\n",
      "432,    Training Loss: 0.507251,    Testing Loss: 0.507251,\n",
      "433,    Training Loss: 0.506428,    Testing Loss: 0.506428,\n",
      "434,    Training Loss: 0.505608,    Testing Loss: 0.505608,\n",
      "435,    Training Loss: 0.504791,    Testing Loss: 0.504791,\n",
      "436,    Training Loss: 0.503979,    Testing Loss: 0.503979,\n",
      "437,    Training Loss: 0.503171,    Testing Loss: 0.503171,\n",
      "438,    Training Loss: 0.502366,    Testing Loss: 0.502366,\n",
      "439,    Training Loss: 0.501565,    Testing Loss: 0.501565,\n",
      "440,    Training Loss: 0.500768,    Testing Loss: 0.500768,\n",
      "441,    Training Loss: 0.499974,    Testing Loss: 0.499974,\n",
      "442,    Training Loss: 0.499185,    Testing Loss: 0.499185,\n",
      "443,    Training Loss: 0.498399,    Testing Loss: 0.498399,\n",
      "444,    Training Loss: 0.497618,    Testing Loss: 0.497618,\n",
      "445,    Training Loss: 0.496840,    Testing Loss: 0.496840,\n",
      "446,    Training Loss: 0.496066,    Testing Loss: 0.496066,\n",
      "447,    Training Loss: 0.495296,    Testing Loss: 0.495296,\n",
      "448,    Training Loss: 0.494530,    Testing Loss: 0.494530,\n",
      "449,    Training Loss: 0.493769,    Testing Loss: 0.493769,\n",
      "450,    Training Loss: 0.493011,    Testing Loss: 0.493011,\n",
      "451,    Training Loss: 0.492257,    Testing Loss: 0.492257,\n",
      "452,    Training Loss: 0.491507,    Testing Loss: 0.491507,\n",
      "453,    Training Loss: 0.490761,    Testing Loss: 0.490761,\n",
      "454,    Training Loss: 0.490019,    Testing Loss: 0.490019,\n",
      "455,    Training Loss: 0.489282,    Testing Loss: 0.489282,\n",
      "456,    Training Loss: 0.488548,    Testing Loss: 0.488548,\n",
      "457,    Training Loss: 0.487818,    Testing Loss: 0.487818,\n",
      "458,    Training Loss: 0.487092,    Testing Loss: 0.487092,\n",
      "459,    Training Loss: 0.486371,    Testing Loss: 0.486371,\n",
      "460,    Training Loss: 0.485653,    Testing Loss: 0.485653,\n",
      "461,    Training Loss: 0.484939,    Testing Loss: 0.484939,\n",
      "462,    Training Loss: 0.484230,    Testing Loss: 0.484230,\n",
      "463,    Training Loss: 0.483524,    Testing Loss: 0.483524,\n",
      "464,    Training Loss: 0.482823,    Testing Loss: 0.482823,\n",
      "465,    Training Loss: 0.482125,    Testing Loss: 0.482125,\n",
      "466,    Training Loss: 0.481432,    Testing Loss: 0.481432,\n",
      "467,    Training Loss: 0.480743,    Testing Loss: 0.480743,\n",
      "468,    Training Loss: 0.480057,    Testing Loss: 0.480057,\n",
      "469,    Training Loss: 0.479376,    Testing Loss: 0.479376,\n",
      "470,    Training Loss: 0.478699,    Testing Loss: 0.478699,\n",
      "471,    Training Loss: 0.478026,    Testing Loss: 0.478026,\n",
      "472,    Training Loss: 0.477356,    Testing Loss: 0.477356,\n",
      "473,    Training Loss: 0.476691,    Testing Loss: 0.476691,\n",
      "474,    Training Loss: 0.476030,    Testing Loss: 0.476030,\n",
      "475,    Training Loss: 0.475372,    Testing Loss: 0.475372,\n",
      "476,    Training Loss: 0.474719,    Testing Loss: 0.474719,\n",
      "477,    Training Loss: 0.474070,    Testing Loss: 0.474070,\n",
      "478,    Training Loss: 0.473424,    Testing Loss: 0.473424,\n",
      "479,    Training Loss: 0.472783,    Testing Loss: 0.472783,\n",
      "480,    Training Loss: 0.472146,    Testing Loss: 0.472146,\n",
      "481,    Training Loss: 0.471512,    Testing Loss: 0.471512,\n",
      "482,    Training Loss: 0.470882,    Testing Loss: 0.470882,\n",
      "483,    Training Loss: 0.470257,    Testing Loss: 0.470257,\n",
      "484,    Training Loss: 0.469635,    Testing Loss: 0.469635,\n",
      "485,    Training Loss: 0.469017,    Testing Loss: 0.469017,\n",
      "486,    Training Loss: 0.468403,    Testing Loss: 0.468403,\n",
      "487,    Training Loss: 0.467793,    Testing Loss: 0.467793,\n",
      "488,    Training Loss: 0.467186,    Testing Loss: 0.467186,\n",
      "489,    Training Loss: 0.466584,    Testing Loss: 0.466584,\n",
      "490,    Training Loss: 0.465985,    Testing Loss: 0.465985,\n",
      "491,    Training Loss: 0.465390,    Testing Loss: 0.465390,\n",
      "492,    Training Loss: 0.464799,    Testing Loss: 0.464799,\n",
      "493,    Training Loss: 0.464211,    Testing Loss: 0.464211,\n",
      "494,    Training Loss: 0.463628,    Testing Loss: 0.463628,\n",
      "495,    Training Loss: 0.463048,    Testing Loss: 0.463048,\n",
      "496,    Training Loss: 0.462471,    Testing Loss: 0.462471,\n",
      "497,    Training Loss: 0.461899,    Testing Loss: 0.461899,\n",
      "498,    Training Loss: 0.461330,    Testing Loss: 0.461330,\n",
      "499,    Training Loss: 0.460765,    Testing Loss: 0.460765,\n",
      "500,    Training Loss: 0.460204,    Testing Loss: 0.460204,\n",
      "501,    Training Loss: 0.459646,    Testing Loss: 0.459646,\n",
      "502,    Training Loss: 0.459091,    Testing Loss: 0.459091,\n",
      "503,    Training Loss: 0.458541,    Testing Loss: 0.458541,\n",
      "504,    Training Loss: 0.457994,    Testing Loss: 0.457994,\n",
      "505,    Training Loss: 0.457450,    Testing Loss: 0.457450,\n",
      "506,    Training Loss: 0.456910,    Testing Loss: 0.456910,\n",
      "507,    Training Loss: 0.456374,    Testing Loss: 0.456374,\n",
      "508,    Training Loss: 0.455841,    Testing Loss: 0.455841,\n",
      "509,    Training Loss: 0.455312,    Testing Loss: 0.455312,\n",
      "510,    Training Loss: 0.454786,    Testing Loss: 0.454786,\n",
      "511,    Training Loss: 0.454263,    Testing Loss: 0.454263,\n",
      "512,    Training Loss: 0.453744,    Testing Loss: 0.453744,\n",
      "513,    Training Loss: 0.453229,    Testing Loss: 0.453229,\n",
      "514,    Training Loss: 0.452716,    Testing Loss: 0.452716,\n",
      "515,    Training Loss: 0.452208,    Testing Loss: 0.452208,\n",
      "516,    Training Loss: 0.451702,    Testing Loss: 0.451702,\n",
      "517,    Training Loss: 0.451200,    Testing Loss: 0.451200,\n",
      "518,    Training Loss: 0.450701,    Testing Loss: 0.450701,\n",
      "519,    Training Loss: 0.450206,    Testing Loss: 0.450206,\n",
      "520,    Training Loss: 0.449713,    Testing Loss: 0.449713,\n",
      "521,    Training Loss: 0.449224,    Testing Loss: 0.449224,\n",
      "522,    Training Loss: 0.448739,    Testing Loss: 0.448739,\n",
      "523,    Training Loss: 0.448256,    Testing Loss: 0.448256,\n",
      "524,    Training Loss: 0.447777,    Testing Loss: 0.447777,\n",
      "525,    Training Loss: 0.447301,    Testing Loss: 0.447301,\n",
      "526,    Training Loss: 0.446828,    Testing Loss: 0.446828,\n",
      "527,    Training Loss: 0.446358,    Testing Loss: 0.446358,\n",
      "528,    Training Loss: 0.445891,    Testing Loss: 0.445891,\n",
      "529,    Training Loss: 0.445428,    Testing Loss: 0.445428,\n",
      "530,    Training Loss: 0.444967,    Testing Loss: 0.444967,\n",
      "531,    Training Loss: 0.444510,    Testing Loss: 0.444510,\n",
      "532,    Training Loss: 0.444055,    Testing Loss: 0.444055,\n",
      "533,    Training Loss: 0.443604,    Testing Loss: 0.443604,\n",
      "534,    Training Loss: 0.443156,    Testing Loss: 0.443156,\n",
      "535,    Training Loss: 0.442710,    Testing Loss: 0.442710,\n",
      "536,    Training Loss: 0.442268,    Testing Loss: 0.442268,\n",
      "537,    Training Loss: 0.441828,    Testing Loss: 0.441828,\n",
      "538,    Training Loss: 0.441392,    Testing Loss: 0.441392,\n",
      "539,    Training Loss: 0.440958,    Testing Loss: 0.440958,\n",
      "540,    Training Loss: 0.440527,    Testing Loss: 0.440527,\n",
      "541,    Training Loss: 0.440099,    Testing Loss: 0.440099,\n",
      "542,    Training Loss: 0.439674,    Testing Loss: 0.439674,\n",
      "543,    Training Loss: 0.439252,    Testing Loss: 0.439252,\n",
      "544,    Training Loss: 0.438833,    Testing Loss: 0.438833,\n",
      "545,    Training Loss: 0.438416,    Testing Loss: 0.438416,\n",
      "546,    Training Loss: 0.438002,    Testing Loss: 0.438002,\n",
      "547,    Training Loss: 0.437591,    Testing Loss: 0.437591,\n",
      "548,    Training Loss: 0.437183,    Testing Loss: 0.437183,\n",
      "549,    Training Loss: 0.436777,    Testing Loss: 0.436777,\n",
      "550,    Training Loss: 0.436374,    Testing Loss: 0.436374,\n",
      "551,    Training Loss: 0.435974,    Testing Loss: 0.435974,\n",
      "552,    Training Loss: 0.435576,    Testing Loss: 0.435576,\n",
      "553,    Training Loss: 0.435181,    Testing Loss: 0.435181,\n",
      "554,    Training Loss: 0.434789,    Testing Loss: 0.434789,\n",
      "555,    Training Loss: 0.434399,    Testing Loss: 0.434399,\n",
      "556,    Training Loss: 0.434012,    Testing Loss: 0.434012,\n",
      "557,    Training Loss: 0.433627,    Testing Loss: 0.433627,\n",
      "558,    Training Loss: 0.433245,    Testing Loss: 0.433245,\n",
      "559,    Training Loss: 0.432866,    Testing Loss: 0.432866,\n",
      "560,    Training Loss: 0.432489,    Testing Loss: 0.432489,\n",
      "561,    Training Loss: 0.432114,    Testing Loss: 0.432114,\n",
      "562,    Training Loss: 0.431742,    Testing Loss: 0.431742,\n",
      "563,    Training Loss: 0.431373,    Testing Loss: 0.431373,\n",
      "564,    Training Loss: 0.431005,    Testing Loss: 0.431005,\n",
      "565,    Training Loss: 0.430641,    Testing Loss: 0.430641,\n",
      "566,    Training Loss: 0.430278,    Testing Loss: 0.430278,\n",
      "567,    Training Loss: 0.429918,    Testing Loss: 0.429918,\n",
      "568,    Training Loss: 0.429560,    Testing Loss: 0.429560,\n",
      "569,    Training Loss: 0.429205,    Testing Loss: 0.429205,\n",
      "570,    Training Loss: 0.428852,    Testing Loss: 0.428852,\n",
      "571,    Training Loss: 0.428502,    Testing Loss: 0.428502,\n",
      "572,    Training Loss: 0.428153,    Testing Loss: 0.428153,\n",
      "573,    Training Loss: 0.427807,    Testing Loss: 0.427807,\n",
      "574,    Training Loss: 0.427463,    Testing Loss: 0.427463,\n",
      "575,    Training Loss: 0.427122,    Testing Loss: 0.427122,\n",
      "576,    Training Loss: 0.426782,    Testing Loss: 0.426782,\n",
      "577,    Training Loss: 0.426445,    Testing Loss: 0.426445,\n",
      "578,    Training Loss: 0.426110,    Testing Loss: 0.426110,\n",
      "579,    Training Loss: 0.425778,    Testing Loss: 0.425778,\n",
      "580,    Training Loss: 0.425447,    Testing Loss: 0.425447,\n",
      "581,    Training Loss: 0.425118,    Testing Loss: 0.425118,\n",
      "582,    Training Loss: 0.424792,    Testing Loss: 0.424792,\n",
      "583,    Training Loss: 0.424468,    Testing Loss: 0.424468,\n",
      "584,    Training Loss: 0.424146,    Testing Loss: 0.424146,\n",
      "585,    Training Loss: 0.423826,    Testing Loss: 0.423826,\n",
      "586,    Training Loss: 0.423508,    Testing Loss: 0.423508,\n",
      "587,    Training Loss: 0.423192,    Testing Loss: 0.423192,\n",
      "588,    Training Loss: 0.422878,    Testing Loss: 0.422878,\n",
      "589,    Training Loss: 0.422566,    Testing Loss: 0.422566,\n",
      "590,    Training Loss: 0.422256,    Testing Loss: 0.422256,\n",
      "591,    Training Loss: 0.421948,    Testing Loss: 0.421948,\n",
      "592,    Training Loss: 0.421642,    Testing Loss: 0.421642,\n",
      "593,    Training Loss: 0.421339,    Testing Loss: 0.421339,\n",
      "594,    Training Loss: 0.421036,    Testing Loss: 0.421036,\n",
      "595,    Training Loss: 0.420736,    Testing Loss: 0.420736,\n",
      "596,    Training Loss: 0.420438,    Testing Loss: 0.420438,\n",
      "597,    Training Loss: 0.420142,    Testing Loss: 0.420142,\n",
      "598,    Training Loss: 0.419848,    Testing Loss: 0.419848,\n",
      "599,    Training Loss: 0.419555,    Testing Loss: 0.419555,\n",
      "600,    Training Loss: 0.419264,    Testing Loss: 0.419264,\n",
      "601,    Training Loss: 0.418976,    Testing Loss: 0.418976,\n",
      "602,    Training Loss: 0.418689,    Testing Loss: 0.418689,\n",
      "603,    Training Loss: 0.418403,    Testing Loss: 0.418403,\n",
      "604,    Training Loss: 0.418120,    Testing Loss: 0.418120,\n",
      "605,    Training Loss: 0.417838,    Testing Loss: 0.417838,\n",
      "606,    Training Loss: 0.417559,    Testing Loss: 0.417559,\n",
      "607,    Training Loss: 0.417280,    Testing Loss: 0.417280,\n",
      "608,    Training Loss: 0.417004,    Testing Loss: 0.417004,\n",
      "609,    Training Loss: 0.416729,    Testing Loss: 0.416729,\n",
      "610,    Training Loss: 0.416457,    Testing Loss: 0.416457,\n",
      "611,    Training Loss: 0.416185,    Testing Loss: 0.416185,\n",
      "612,    Training Loss: 0.415916,    Testing Loss: 0.415916,\n",
      "613,    Training Loss: 0.415648,    Testing Loss: 0.415648,\n",
      "614,    Training Loss: 0.415382,    Testing Loss: 0.415382,\n",
      "615,    Training Loss: 0.415118,    Testing Loss: 0.415118,\n",
      "616,    Training Loss: 0.414855,    Testing Loss: 0.414855,\n",
      "617,    Training Loss: 0.414594,    Testing Loss: 0.414594,\n",
      "618,    Training Loss: 0.414334,    Testing Loss: 0.414334,\n",
      "619,    Training Loss: 0.414076,    Testing Loss: 0.414076,\n",
      "620,    Training Loss: 0.413820,    Testing Loss: 0.413820,\n",
      "621,    Training Loss: 0.413565,    Testing Loss: 0.413565,\n",
      "622,    Training Loss: 0.413312,    Testing Loss: 0.413312,\n",
      "623,    Training Loss: 0.413060,    Testing Loss: 0.413060,\n",
      "624,    Training Loss: 0.412810,    Testing Loss: 0.412810,\n",
      "625,    Training Loss: 0.412561,    Testing Loss: 0.412561,\n",
      "626,    Training Loss: 0.412314,    Testing Loss: 0.412314,\n",
      "627,    Training Loss: 0.412068,    Testing Loss: 0.412068,\n",
      "628,    Training Loss: 0.411824,    Testing Loss: 0.411824,\n",
      "629,    Training Loss: 0.411582,    Testing Loss: 0.411582,\n",
      "630,    Training Loss: 0.411341,    Testing Loss: 0.411341,\n",
      "631,    Training Loss: 0.411101,    Testing Loss: 0.411101,\n",
      "632,    Training Loss: 0.410863,    Testing Loss: 0.410863,\n",
      "633,    Training Loss: 0.410626,    Testing Loss: 0.410626,\n",
      "634,    Training Loss: 0.410390,    Testing Loss: 0.410390,\n",
      "635,    Training Loss: 0.410157,    Testing Loss: 0.410157,\n",
      "636,    Training Loss: 0.409924,    Testing Loss: 0.409924,\n",
      "637,    Training Loss: 0.409693,    Testing Loss: 0.409693,\n",
      "638,    Training Loss: 0.409463,    Testing Loss: 0.409463,\n",
      "639,    Training Loss: 0.409235,    Testing Loss: 0.409235,\n",
      "640,    Training Loss: 0.409008,    Testing Loss: 0.409008,\n",
      "641,    Training Loss: 0.408782,    Testing Loss: 0.408782,\n",
      "642,    Training Loss: 0.408558,    Testing Loss: 0.408558,\n",
      "643,    Training Loss: 0.408335,    Testing Loss: 0.408335,\n",
      "644,    Training Loss: 0.408113,    Testing Loss: 0.408113,\n",
      "645,    Training Loss: 0.407893,    Testing Loss: 0.407893,\n",
      "646,    Training Loss: 0.407674,    Testing Loss: 0.407674,\n",
      "647,    Training Loss: 0.407456,    Testing Loss: 0.407456,\n",
      "648,    Training Loss: 0.407240,    Testing Loss: 0.407240,\n",
      "649,    Training Loss: 0.407025,    Testing Loss: 0.407025,\n",
      "650,    Training Loss: 0.406810,    Testing Loss: 0.406810,\n",
      "651,    Training Loss: 0.406598,    Testing Loss: 0.406598,\n",
      "652,    Training Loss: 0.406386,    Testing Loss: 0.406386,\n",
      "653,    Training Loss: 0.406176,    Testing Loss: 0.406176,\n",
      "654,    Training Loss: 0.405967,    Testing Loss: 0.405967,\n",
      "655,    Training Loss: 0.405760,    Testing Loss: 0.405760,\n",
      "656,    Training Loss: 0.405553,    Testing Loss: 0.405553,\n",
      "657,    Training Loss: 0.405348,    Testing Loss: 0.405348,\n",
      "658,    Training Loss: 0.405144,    Testing Loss: 0.405144,\n",
      "659,    Training Loss: 0.404941,    Testing Loss: 0.404941,\n",
      "660,    Training Loss: 0.404739,    Testing Loss: 0.404739,\n",
      "661,    Training Loss: 0.404538,    Testing Loss: 0.404538,\n",
      "662,    Training Loss: 0.404339,    Testing Loss: 0.404339,\n",
      "663,    Training Loss: 0.404141,    Testing Loss: 0.404141,\n",
      "664,    Training Loss: 0.403944,    Testing Loss: 0.403944,\n",
      "665,    Training Loss: 0.403747,    Testing Loss: 0.403747,\n",
      "666,    Training Loss: 0.403553,    Testing Loss: 0.403553,\n",
      "667,    Training Loss: 0.403359,    Testing Loss: 0.403359,\n",
      "668,    Training Loss: 0.403166,    Testing Loss: 0.403166,\n",
      "669,    Training Loss: 0.402975,    Testing Loss: 0.402975,\n",
      "670,    Training Loss: 0.402784,    Testing Loss: 0.402784,\n",
      "671,    Training Loss: 0.402595,    Testing Loss: 0.402595,\n",
      "672,    Training Loss: 0.402406,    Testing Loss: 0.402406,\n",
      "673,    Training Loss: 0.402219,    Testing Loss: 0.402219,\n",
      "674,    Training Loss: 0.402033,    Testing Loss: 0.402033,\n",
      "675,    Training Loss: 0.401848,    Testing Loss: 0.401848,\n",
      "676,    Training Loss: 0.401664,    Testing Loss: 0.401664,\n",
      "677,    Training Loss: 0.401480,    Testing Loss: 0.401480,\n",
      "678,    Training Loss: 0.401299,    Testing Loss: 0.401299,\n",
      "679,    Training Loss: 0.401117,    Testing Loss: 0.401117,\n",
      "680,    Training Loss: 0.400937,    Testing Loss: 0.400937,\n",
      "681,    Training Loss: 0.400758,    Testing Loss: 0.400758,\n",
      "682,    Training Loss: 0.400580,    Testing Loss: 0.400580,\n",
      "683,    Training Loss: 0.400403,    Testing Loss: 0.400403,\n",
      "684,    Training Loss: 0.400227,    Testing Loss: 0.400227,\n",
      "685,    Training Loss: 0.400052,    Testing Loss: 0.400052,\n",
      "686,    Training Loss: 0.399878,    Testing Loss: 0.399878,\n",
      "687,    Training Loss: 0.399705,    Testing Loss: 0.399705,\n",
      "688,    Training Loss: 0.399533,    Testing Loss: 0.399533,\n",
      "689,    Training Loss: 0.399361,    Testing Loss: 0.399361,\n",
      "690,    Training Loss: 0.399191,    Testing Loss: 0.399191,\n",
      "691,    Training Loss: 0.399022,    Testing Loss: 0.399022,\n",
      "692,    Training Loss: 0.398853,    Testing Loss: 0.398853,\n",
      "693,    Training Loss: 0.398686,    Testing Loss: 0.398686,\n",
      "694,    Training Loss: 0.398519,    Testing Loss: 0.398519,\n",
      "695,    Training Loss: 0.398353,    Testing Loss: 0.398353,\n",
      "696,    Training Loss: 0.398189,    Testing Loss: 0.398189,\n",
      "697,    Training Loss: 0.398025,    Testing Loss: 0.398025,\n",
      "698,    Training Loss: 0.397862,    Testing Loss: 0.397862,\n",
      "699,    Training Loss: 0.397700,    Testing Loss: 0.397700,\n",
      "700,    Training Loss: 0.397538,    Testing Loss: 0.397538,\n",
      "701,    Training Loss: 0.397378,    Testing Loss: 0.397378,\n",
      "702,    Training Loss: 0.397218,    Testing Loss: 0.397218,\n",
      "703,    Training Loss: 0.397060,    Testing Loss: 0.397060,\n",
      "704,    Training Loss: 0.396902,    Testing Loss: 0.396902,\n",
      "705,    Training Loss: 0.396745,    Testing Loss: 0.396745,\n",
      "706,    Training Loss: 0.396589,    Testing Loss: 0.396589,\n",
      "707,    Training Loss: 0.396433,    Testing Loss: 0.396433,\n",
      "708,    Training Loss: 0.396279,    Testing Loss: 0.396279,\n",
      "709,    Training Loss: 0.396125,    Testing Loss: 0.396125,\n",
      "710,    Training Loss: 0.395972,    Testing Loss: 0.395972,\n",
      "711,    Training Loss: 0.395820,    Testing Loss: 0.395820,\n",
      "712,    Training Loss: 0.395669,    Testing Loss: 0.395669,\n",
      "713,    Training Loss: 0.395519,    Testing Loss: 0.395519,\n",
      "714,    Training Loss: 0.395369,    Testing Loss: 0.395369,\n",
      "715,    Training Loss: 0.395220,    Testing Loss: 0.395220,\n",
      "716,    Training Loss: 0.395072,    Testing Loss: 0.395072,\n",
      "717,    Training Loss: 0.394925,    Testing Loss: 0.394925,\n",
      "718,    Training Loss: 0.394778,    Testing Loss: 0.394778,\n",
      "719,    Training Loss: 0.394633,    Testing Loss: 0.394633,\n",
      "720,    Training Loss: 0.394487,    Testing Loss: 0.394487,\n",
      "721,    Training Loss: 0.394343,    Testing Loss: 0.394343,\n",
      "722,    Training Loss: 0.394200,    Testing Loss: 0.394200,\n",
      "723,    Training Loss: 0.394057,    Testing Loss: 0.394057,\n",
      "724,    Training Loss: 0.393915,    Testing Loss: 0.393915,\n",
      "725,    Training Loss: 0.393774,    Testing Loss: 0.393774,\n",
      "726,    Training Loss: 0.393633,    Testing Loss: 0.393633,\n",
      "727,    Training Loss: 0.393493,    Testing Loss: 0.393493,\n",
      "728,    Training Loss: 0.393354,    Testing Loss: 0.393354,\n",
      "729,    Training Loss: 0.393216,    Testing Loss: 0.393216,\n",
      "730,    Training Loss: 0.393078,    Testing Loss: 0.393078,\n",
      "731,    Training Loss: 0.392941,    Testing Loss: 0.392941,\n",
      "732,    Training Loss: 0.392805,    Testing Loss: 0.392805,\n",
      "733,    Training Loss: 0.392669,    Testing Loss: 0.392669,\n",
      "734,    Training Loss: 0.392534,    Testing Loss: 0.392534,\n",
      "735,    Training Loss: 0.392400,    Testing Loss: 0.392400,\n",
      "736,    Training Loss: 0.392266,    Testing Loss: 0.392266,\n",
      "737,    Training Loss: 0.392134,    Testing Loss: 0.392134,\n",
      "738,    Training Loss: 0.392001,    Testing Loss: 0.392001,\n",
      "739,    Training Loss: 0.391870,    Testing Loss: 0.391870,\n",
      "740,    Training Loss: 0.391739,    Testing Loss: 0.391739,\n",
      "741,    Training Loss: 0.391609,    Testing Loss: 0.391609,\n",
      "742,    Training Loss: 0.391479,    Testing Loss: 0.391479,\n",
      "743,    Training Loss: 0.391350,    Testing Loss: 0.391350,\n",
      "744,    Training Loss: 0.391222,    Testing Loss: 0.391222,\n",
      "745,    Training Loss: 0.391094,    Testing Loss: 0.391094,\n",
      "746,    Training Loss: 0.390967,    Testing Loss: 0.390967,\n",
      "747,    Training Loss: 0.390841,    Testing Loss: 0.390841,\n",
      "748,    Training Loss: 0.390715,    Testing Loss: 0.390715,\n",
      "749,    Training Loss: 0.390590,    Testing Loss: 0.390590,\n",
      "750,    Training Loss: 0.390465,    Testing Loss: 0.390465,\n",
      "751,    Training Loss: 0.390341,    Testing Loss: 0.390341,\n",
      "752,    Training Loss: 0.390218,    Testing Loss: 0.390218,\n",
      "753,    Training Loss: 0.390095,    Testing Loss: 0.390095,\n",
      "754,    Training Loss: 0.389973,    Testing Loss: 0.389973,\n",
      "755,    Training Loss: 0.389852,    Testing Loss: 0.389852,\n",
      "756,    Training Loss: 0.389731,    Testing Loss: 0.389731,\n",
      "757,    Training Loss: 0.389611,    Testing Loss: 0.389611,\n",
      "758,    Training Loss: 0.389491,    Testing Loss: 0.389491,\n",
      "759,    Training Loss: 0.389372,    Testing Loss: 0.389372,\n",
      "760,    Training Loss: 0.389253,    Testing Loss: 0.389253,\n",
      "761,    Training Loss: 0.389135,    Testing Loss: 0.389135,\n",
      "762,    Training Loss: 0.389018,    Testing Loss: 0.389018,\n",
      "763,    Training Loss: 0.388901,    Testing Loss: 0.388901,\n",
      "764,    Training Loss: 0.388785,    Testing Loss: 0.388785,\n",
      "765,    Training Loss: 0.388669,    Testing Loss: 0.388669,\n",
      "766,    Training Loss: 0.388554,    Testing Loss: 0.388554,\n",
      "767,    Training Loss: 0.388439,    Testing Loss: 0.388439,\n",
      "768,    Training Loss: 0.388325,    Testing Loss: 0.388325,\n",
      "769,    Training Loss: 0.388212,    Testing Loss: 0.388212,\n",
      "770,    Training Loss: 0.388099,    Testing Loss: 0.388099,\n",
      "771,    Training Loss: 0.387986,    Testing Loss: 0.387986,\n",
      "772,    Training Loss: 0.387874,    Testing Loss: 0.387874,\n",
      "773,    Training Loss: 0.387763,    Testing Loss: 0.387763,\n",
      "774,    Training Loss: 0.387652,    Testing Loss: 0.387652,\n",
      "775,    Training Loss: 0.387542,    Testing Loss: 0.387542,\n",
      "776,    Training Loss: 0.387432,    Testing Loss: 0.387432,\n",
      "777,    Training Loss: 0.387323,    Testing Loss: 0.387323,\n",
      "778,    Training Loss: 0.387214,    Testing Loss: 0.387214,\n",
      "779,    Training Loss: 0.387106,    Testing Loss: 0.387106,\n",
      "780,    Training Loss: 0.386998,    Testing Loss: 0.386998,\n",
      "781,    Training Loss: 0.386891,    Testing Loss: 0.386891,\n",
      "782,    Training Loss: 0.386784,    Testing Loss: 0.386784,\n",
      "783,    Training Loss: 0.386678,    Testing Loss: 0.386678,\n",
      "784,    Training Loss: 0.386572,    Testing Loss: 0.386572,\n",
      "785,    Training Loss: 0.386467,    Testing Loss: 0.386467,\n",
      "786,    Training Loss: 0.386362,    Testing Loss: 0.386362,\n",
      "787,    Training Loss: 0.386258,    Testing Loss: 0.386258,\n",
      "788,    Training Loss: 0.386154,    Testing Loss: 0.386154,\n",
      "789,    Training Loss: 0.386050,    Testing Loss: 0.386050,\n",
      "790,    Training Loss: 0.385948,    Testing Loss: 0.385948,\n",
      "791,    Training Loss: 0.385845,    Testing Loss: 0.385845,\n",
      "792,    Training Loss: 0.385743,    Testing Loss: 0.385743,\n",
      "793,    Training Loss: 0.385642,    Testing Loss: 0.385642,\n",
      "794,    Training Loss: 0.385541,    Testing Loss: 0.385541,\n",
      "795,    Training Loss: 0.385440,    Testing Loss: 0.385440,\n",
      "796,    Training Loss: 0.385340,    Testing Loss: 0.385340,\n",
      "797,    Training Loss: 0.385241,    Testing Loss: 0.385241,\n",
      "798,    Training Loss: 0.385142,    Testing Loss: 0.385142,\n",
      "799,    Training Loss: 0.385043,    Testing Loss: 0.385043,\n",
      "800,    Training Loss: 0.384945,    Testing Loss: 0.384945,\n",
      "801,    Training Loss: 0.384847,    Testing Loss: 0.384847,\n",
      "802,    Training Loss: 0.384749,    Testing Loss: 0.384749,\n",
      "803,    Training Loss: 0.384652,    Testing Loss: 0.384652,\n",
      "804,    Training Loss: 0.384556,    Testing Loss: 0.384556,\n",
      "805,    Training Loss: 0.384460,    Testing Loss: 0.384460,\n",
      "806,    Training Loss: 0.384364,    Testing Loss: 0.384364,\n",
      "807,    Training Loss: 0.384269,    Testing Loss: 0.384269,\n",
      "808,    Training Loss: 0.384174,    Testing Loss: 0.384174,\n",
      "809,    Training Loss: 0.384080,    Testing Loss: 0.384080,\n",
      "810,    Training Loss: 0.383986,    Testing Loss: 0.383986,\n",
      "811,    Training Loss: 0.383892,    Testing Loss: 0.383892,\n",
      "812,    Training Loss: 0.383799,    Testing Loss: 0.383799,\n",
      "813,    Training Loss: 0.383707,    Testing Loss: 0.383707,\n",
      "814,    Training Loss: 0.383614,    Testing Loss: 0.383614,\n",
      "815,    Training Loss: 0.383522,    Testing Loss: 0.383522,\n",
      "816,    Training Loss: 0.383431,    Testing Loss: 0.383431,\n",
      "817,    Training Loss: 0.383340,    Testing Loss: 0.383340,\n",
      "818,    Training Loss: 0.383249,    Testing Loss: 0.383249,\n",
      "819,    Training Loss: 0.383159,    Testing Loss: 0.383159,\n",
      "820,    Training Loss: 0.383069,    Testing Loss: 0.383069,\n",
      "821,    Training Loss: 0.382980,    Testing Loss: 0.382980,\n",
      "822,    Training Loss: 0.382891,    Testing Loss: 0.382891,\n",
      "823,    Training Loss: 0.382802,    Testing Loss: 0.382802,\n",
      "824,    Training Loss: 0.382713,    Testing Loss: 0.382713,\n",
      "825,    Training Loss: 0.382626,    Testing Loss: 0.382626,\n",
      "826,    Training Loss: 0.382538,    Testing Loss: 0.382538,\n",
      "827,    Training Loss: 0.382451,    Testing Loss: 0.382451,\n",
      "828,    Training Loss: 0.382364,    Testing Loss: 0.382364,\n",
      "829,    Training Loss: 0.382278,    Testing Loss: 0.382278,\n",
      "830,    Training Loss: 0.382192,    Testing Loss: 0.382192,\n",
      "831,    Training Loss: 0.382106,    Testing Loss: 0.382106,\n",
      "832,    Training Loss: 0.382020,    Testing Loss: 0.382020,\n",
      "833,    Training Loss: 0.381936,    Testing Loss: 0.381936,\n",
      "834,    Training Loss: 0.381851,    Testing Loss: 0.381851,\n",
      "835,    Training Loss: 0.381767,    Testing Loss: 0.381767,\n",
      "836,    Training Loss: 0.381683,    Testing Loss: 0.381683,\n",
      "837,    Training Loss: 0.381599,    Testing Loss: 0.381599,\n",
      "838,    Training Loss: 0.381516,    Testing Loss: 0.381516,\n",
      "839,    Training Loss: 0.381433,    Testing Loss: 0.381433,\n",
      "840,    Training Loss: 0.381351,    Testing Loss: 0.381351,\n",
      "841,    Training Loss: 0.381269,    Testing Loss: 0.381269,\n",
      "842,    Training Loss: 0.381187,    Testing Loss: 0.381187,\n",
      "843,    Training Loss: 0.381106,    Testing Loss: 0.381106,\n",
      "844,    Training Loss: 0.381025,    Testing Loss: 0.381025,\n",
      "845,    Training Loss: 0.380944,    Testing Loss: 0.380944,\n",
      "846,    Training Loss: 0.380864,    Testing Loss: 0.380864,\n",
      "847,    Training Loss: 0.380783,    Testing Loss: 0.380783,\n",
      "848,    Training Loss: 0.380704,    Testing Loss: 0.380704,\n",
      "849,    Training Loss: 0.380624,    Testing Loss: 0.380624,\n",
      "850,    Training Loss: 0.380545,    Testing Loss: 0.380545,\n",
      "851,    Training Loss: 0.380467,    Testing Loss: 0.380467,\n",
      "852,    Training Loss: 0.380388,    Testing Loss: 0.380388,\n",
      "853,    Training Loss: 0.380310,    Testing Loss: 0.380310,\n",
      "854,    Training Loss: 0.380232,    Testing Loss: 0.380232,\n",
      "855,    Training Loss: 0.380155,    Testing Loss: 0.380155,\n",
      "856,    Training Loss: 0.380078,    Testing Loss: 0.380078,\n",
      "857,    Training Loss: 0.380001,    Testing Loss: 0.380001,\n",
      "858,    Training Loss: 0.379924,    Testing Loss: 0.379924,\n",
      "859,    Training Loss: 0.379848,    Testing Loss: 0.379848,\n",
      "860,    Training Loss: 0.379772,    Testing Loss: 0.379772,\n",
      "861,    Training Loss: 0.379697,    Testing Loss: 0.379697,\n",
      "862,    Training Loss: 0.379622,    Testing Loss: 0.379622,\n",
      "863,    Training Loss: 0.379547,    Testing Loss: 0.379547,\n",
      "864,    Training Loss: 0.379472,    Testing Loss: 0.379472,\n",
      "865,    Training Loss: 0.379398,    Testing Loss: 0.379398,\n",
      "866,    Training Loss: 0.379324,    Testing Loss: 0.379324,\n",
      "867,    Training Loss: 0.379250,    Testing Loss: 0.379250,\n",
      "868,    Training Loss: 0.379177,    Testing Loss: 0.379177,\n",
      "869,    Training Loss: 0.379103,    Testing Loss: 0.379103,\n",
      "870,    Training Loss: 0.379030,    Testing Loss: 0.379030,\n",
      "871,    Training Loss: 0.378958,    Testing Loss: 0.378958,\n",
      "872,    Training Loss: 0.378886,    Testing Loss: 0.378886,\n",
      "873,    Training Loss: 0.378814,    Testing Loss: 0.378814,\n",
      "874,    Training Loss: 0.378742,    Testing Loss: 0.378742,\n",
      "875,    Training Loss: 0.378671,    Testing Loss: 0.378671,\n",
      "876,    Training Loss: 0.378600,    Testing Loss: 0.378600,\n",
      "877,    Training Loss: 0.378529,    Testing Loss: 0.378529,\n",
      "878,    Training Loss: 0.378458,    Testing Loss: 0.378458,\n",
      "879,    Training Loss: 0.378388,    Testing Loss: 0.378388,\n",
      "880,    Training Loss: 0.378318,    Testing Loss: 0.378318,\n",
      "881,    Training Loss: 0.378248,    Testing Loss: 0.378248,\n",
      "882,    Training Loss: 0.378179,    Testing Loss: 0.378179,\n",
      "883,    Training Loss: 0.378110,    Testing Loss: 0.378110,\n",
      "884,    Training Loss: 0.378041,    Testing Loss: 0.378041,\n",
      "885,    Training Loss: 0.377972,    Testing Loss: 0.377972,\n",
      "886,    Training Loss: 0.377904,    Testing Loss: 0.377904,\n",
      "887,    Training Loss: 0.377836,    Testing Loss: 0.377836,\n",
      "888,    Training Loss: 0.377768,    Testing Loss: 0.377768,\n",
      "889,    Training Loss: 0.377701,    Testing Loss: 0.377701,\n",
      "890,    Training Loss: 0.377633,    Testing Loss: 0.377633,\n",
      "891,    Training Loss: 0.377566,    Testing Loss: 0.377566,\n",
      "892,    Training Loss: 0.377500,    Testing Loss: 0.377500,\n",
      "893,    Training Loss: 0.377433,    Testing Loss: 0.377433,\n",
      "894,    Training Loss: 0.377367,    Testing Loss: 0.377367,\n",
      "895,    Training Loss: 0.377301,    Testing Loss: 0.377301,\n",
      "896,    Training Loss: 0.377235,    Testing Loss: 0.377235,\n",
      "897,    Training Loss: 0.377170,    Testing Loss: 0.377170,\n",
      "898,    Training Loss: 0.377104,    Testing Loss: 0.377104,\n",
      "899,    Training Loss: 0.377040,    Testing Loss: 0.377040,\n",
      "900,    Training Loss: 0.376975,    Testing Loss: 0.376975,\n",
      "901,    Training Loss: 0.376910,    Testing Loss: 0.376910,\n",
      "902,    Training Loss: 0.376846,    Testing Loss: 0.376846,\n",
      "903,    Training Loss: 0.376782,    Testing Loss: 0.376782,\n",
      "904,    Training Loss: 0.376718,    Testing Loss: 0.376718,\n",
      "905,    Training Loss: 0.376655,    Testing Loss: 0.376655,\n",
      "906,    Training Loss: 0.376592,    Testing Loss: 0.376592,\n",
      "907,    Training Loss: 0.376529,    Testing Loss: 0.376529,\n",
      "908,    Training Loss: 0.376466,    Testing Loss: 0.376466,\n",
      "909,    Training Loss: 0.376403,    Testing Loss: 0.376403,\n",
      "910,    Training Loss: 0.376341,    Testing Loss: 0.376341,\n",
      "911,    Training Loss: 0.376279,    Testing Loss: 0.376279,\n",
      "912,    Training Loss: 0.376217,    Testing Loss: 0.376217,\n",
      "913,    Training Loss: 0.376156,    Testing Loss: 0.376156,\n",
      "914,    Training Loss: 0.376094,    Testing Loss: 0.376094,\n",
      "915,    Training Loss: 0.376033,    Testing Loss: 0.376033,\n",
      "916,    Training Loss: 0.375972,    Testing Loss: 0.375972,\n",
      "917,    Training Loss: 0.375911,    Testing Loss: 0.375911,\n",
      "918,    Training Loss: 0.375851,    Testing Loss: 0.375851,\n",
      "919,    Training Loss: 0.375791,    Testing Loss: 0.375791,\n",
      "920,    Training Loss: 0.375731,    Testing Loss: 0.375731,\n",
      "921,    Training Loss: 0.375671,    Testing Loss: 0.375671,\n",
      "922,    Training Loss: 0.375611,    Testing Loss: 0.375611,\n",
      "923,    Training Loss: 0.375552,    Testing Loss: 0.375552,\n",
      "924,    Training Loss: 0.375493,    Testing Loss: 0.375493,\n",
      "925,    Training Loss: 0.375434,    Testing Loss: 0.375434,\n",
      "926,    Training Loss: 0.375376,    Testing Loss: 0.375376,\n",
      "927,    Training Loss: 0.375317,    Testing Loss: 0.375317,\n",
      "928,    Training Loss: 0.375259,    Testing Loss: 0.375259,\n",
      "929,    Training Loss: 0.375201,    Testing Loss: 0.375201,\n",
      "930,    Training Loss: 0.375143,    Testing Loss: 0.375143,\n",
      "931,    Training Loss: 0.375085,    Testing Loss: 0.375085,\n",
      "932,    Training Loss: 0.375028,    Testing Loss: 0.375028,\n",
      "933,    Training Loss: 0.374971,    Testing Loss: 0.374971,\n",
      "934,    Training Loss: 0.374914,    Testing Loss: 0.374914,\n",
      "935,    Training Loss: 0.374857,    Testing Loss: 0.374857,\n",
      "936,    Training Loss: 0.374801,    Testing Loss: 0.374801,\n",
      "937,    Training Loss: 0.374744,    Testing Loss: 0.374744,\n",
      "938,    Training Loss: 0.374688,    Testing Loss: 0.374688,\n",
      "939,    Training Loss: 0.374632,    Testing Loss: 0.374632,\n",
      "940,    Training Loss: 0.374577,    Testing Loss: 0.374577,\n",
      "941,    Training Loss: 0.374521,    Testing Loss: 0.374521,\n",
      "942,    Training Loss: 0.374466,    Testing Loss: 0.374466,\n",
      "943,    Training Loss: 0.374411,    Testing Loss: 0.374411,\n",
      "944,    Training Loss: 0.374355,    Testing Loss: 0.374355,\n",
      "945,    Training Loss: 0.374301,    Testing Loss: 0.374301,\n",
      "946,    Training Loss: 0.374246,    Testing Loss: 0.374246,\n",
      "947,    Training Loss: 0.374192,    Testing Loss: 0.374192,\n",
      "948,    Training Loss: 0.374138,    Testing Loss: 0.374138,\n",
      "949,    Training Loss: 0.374084,    Testing Loss: 0.374084,\n",
      "950,    Training Loss: 0.374030,    Testing Loss: 0.374030,\n",
      "951,    Training Loss: 0.373976,    Testing Loss: 0.373976,\n",
      "952,    Training Loss: 0.373923,    Testing Loss: 0.373923,\n",
      "953,    Training Loss: 0.373870,    Testing Loss: 0.373870,\n",
      "954,    Training Loss: 0.373817,    Testing Loss: 0.373817,\n",
      "955,    Training Loss: 0.373764,    Testing Loss: 0.373764,\n",
      "956,    Training Loss: 0.373711,    Testing Loss: 0.373711,\n",
      "957,    Training Loss: 0.373659,    Testing Loss: 0.373659,\n",
      "958,    Training Loss: 0.373607,    Testing Loss: 0.373607,\n",
      "959,    Training Loss: 0.373555,    Testing Loss: 0.373555,\n",
      "960,    Training Loss: 0.373502,    Testing Loss: 0.373502,\n",
      "961,    Training Loss: 0.373451,    Testing Loss: 0.373451,\n",
      "962,    Training Loss: 0.373399,    Testing Loss: 0.373399,\n",
      "963,    Training Loss: 0.373348,    Testing Loss: 0.373348,\n",
      "964,    Training Loss: 0.373297,    Testing Loss: 0.373297,\n",
      "965,    Training Loss: 0.373246,    Testing Loss: 0.373246,\n",
      "966,    Training Loss: 0.373195,    Testing Loss: 0.373195,\n",
      "967,    Training Loss: 0.373144,    Testing Loss: 0.373144,\n",
      "968,    Training Loss: 0.373094,    Testing Loss: 0.373094,\n",
      "969,    Training Loss: 0.373044,    Testing Loss: 0.373044,\n",
      "970,    Training Loss: 0.372993,    Testing Loss: 0.372993,\n",
      "971,    Training Loss: 0.372944,    Testing Loss: 0.372944,\n",
      "972,    Training Loss: 0.372894,    Testing Loss: 0.372894,\n",
      "973,    Training Loss: 0.372844,    Testing Loss: 0.372844,\n",
      "974,    Training Loss: 0.372795,    Testing Loss: 0.372795,\n",
      "975,    Training Loss: 0.372745,    Testing Loss: 0.372745,\n",
      "976,    Training Loss: 0.372696,    Testing Loss: 0.372696,\n",
      "977,    Training Loss: 0.372647,    Testing Loss: 0.372647,\n",
      "978,    Training Loss: 0.372599,    Testing Loss: 0.372599,\n",
      "979,    Training Loss: 0.372550,    Testing Loss: 0.372550,\n",
      "980,    Training Loss: 0.372502,    Testing Loss: 0.372502,\n",
      "981,    Training Loss: 0.372453,    Testing Loss: 0.372453,\n",
      "982,    Training Loss: 0.372405,    Testing Loss: 0.372405,\n",
      "983,    Training Loss: 0.372357,    Testing Loss: 0.372357,\n",
      "984,    Training Loss: 0.372310,    Testing Loss: 0.372310,\n",
      "985,    Training Loss: 0.372262,    Testing Loss: 0.372262,\n",
      "986,    Training Loss: 0.372215,    Testing Loss: 0.372215,\n",
      "987,    Training Loss: 0.372167,    Testing Loss: 0.372167,\n",
      "988,    Training Loss: 0.372120,    Testing Loss: 0.372120,\n",
      "989,    Training Loss: 0.372073,    Testing Loss: 0.372073,\n",
      "990,    Training Loss: 0.372026,    Testing Loss: 0.372026,\n",
      "991,    Training Loss: 0.371980,    Testing Loss: 0.371980,\n",
      "992,    Training Loss: 0.371933,    Testing Loss: 0.371933,\n",
      "993,    Training Loss: 0.371887,    Testing Loss: 0.371887,\n",
      "994,    Training Loss: 0.371841,    Testing Loss: 0.371841,\n",
      "995,    Training Loss: 0.371795,    Testing Loss: 0.371795,\n",
      "996,    Training Loss: 0.371749,    Testing Loss: 0.371749,\n",
      "997,    Training Loss: 0.371703,    Testing Loss: 0.371703,\n",
      "998,    Training Loss: 0.371657,    Testing Loss: 0.371657,\n",
      "999,    Training Loss: 0.371612,    Testing Loss: 0.371612,\n",
      "1000,    Training Loss: 0.371612,    Testing Loss: 0.371612,\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "( x_train, y_train ), ( x_test, y_test ) = load_data( \"xor\" );\n",
    "\n",
    "\n",
    "\n",
    "# Define activation functions\n",
    "sigmoid = tf.nn.sigmoid\n",
    "softmax = tf.nn.softmax\n",
    "\n",
    "# Define initializers\n",
    "vs_init = tf.initializers.GlorotUniform()  # unseeded\n",
    "zeros_init = tf.zeros\n",
    "DTYPE = tf.float32\n",
    "\n",
    "# Define learning rate\n",
    "learning_rate = 0.5\n",
    "\n",
    "# Define parameters\n",
    "PARAMETERS1 = {\n",
    "    'WEIGHT_INIT_NAME': 'vs_init',\n",
    "    'LOGFILE_NAME': 'Log_trial:',\n",
    "    'WEIGHT_INIT': vs_init,\n",
    "    'BIAS_INIT': zeros_init,\n",
    "    'N_INPUTS': 2,\n",
    "    'N_HIDDEN': 2,\n",
    "    'N_OUTPUTS': 1,\n",
    "    'LEARNING_RATE': learning_rate,\n",
    "    'MAX_EPOCHS': 1000,  # maximum number of epochs to run\n",
    "    'TRAINING_SIZE': x_train.shape[0],  # number of training examples\n",
    "    'TEST_SIZE': x_test.shape[0],  # number of testing examples\n",
    "    'MAX_EXPERIMENTS': 1,\n",
    "    'MINI_BATCH': 1,  # full batch\n",
    "    'DTYPE': tf.float32\n",
    "}\n",
    "\n",
    "with tf.device('/device:GPU:1'):\n",
    "    @tf.function\n",
    "    def loss_function( y_target, y_pred ):\n",
    "        return -tf.reduce_mean( y_target * tf.math.log(y_pred + EPS) + (1 - y_target) * tf.math.log(1 - y_pred + EPS) );\n",
    "\n",
    "# Run experiments\n",
    "for trial in range(PARAMETERS1['MAX_EXPERIMENTS']):\n",
    "    vs_init = tf.initializers.GlorotUniform(seed=trial)\n",
    "    PARAMETERS1['WEIGHT_INIT'] = vs_init\n",
    "\n",
    "    # Define layers\n",
    "    input_l = InputLayer(PARAMETERS1, \"input\", PARAMETERS1['N_INPUTS'])\n",
    "    hidden_l = Layer(PARAMETERS1, \"hidden\", PARAMETERS1['N_HIDDEN'], sigmoid)\n",
    "    output_l = Layer(PARAMETERS1, \"output\", PARAMETERS1['N_OUTPUTS'], sigmoid)\n",
    "\n",
    "    # Define network\n",
    "    nn = input_l >> hidden_l >> output_l >> loss_function;\n",
    "    \n",
    "    learning_algorithm = \"GD\"\n",
    "    algorithm_name = \"GD\"\n",
    "    nn = nn >> learning_algorithm\n",
    "\n",
    "    # Train and reset\n",
    "    logs = nn.train_epochs(y_train, x_train, y_test, x_test, show=True)\n",
    "    nn.reset()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
